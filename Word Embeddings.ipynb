{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(1000,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([1,2,3]))  # 알아서 1,2,3 번 들고나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LeeWonSeok\\\\.keras\\\\datasets'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "\n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)  \n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)  \n",
    "# reshuffle_each_iteration=False iter 할때마다 shffle안하는듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=89, shape=(), dtype=string, numpy=b'It was when they were doing the last part of the course on their way'>, <tf.Tensor: id=90, shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: id=91, shape=(), dtype=string, numpy=b\"Enrag'd, Achilles Hector shall subdue;\">, <tf.Tensor: id=92, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=93, shape=(), dtype=string, numpy=b'They all shall fight; and if thou fail, shalt know'>, <tf.Tensor: id=94, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=95, shape=(), dtype=string, numpy=b'Saved him, and with the golden \\xc3\\xa6gis broad'>, <tf.Tensor: id=96, shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: id=97, shape=(), dtype=string, numpy=b'and killed Iphidamas by striking him on the neck. So there the poor'>, <tf.Tensor: id=98, shape=(), dtype=int64, numpy=2>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode text lines as numbers\n",
    "\n",
    "Machine learning models work on numbers, not words, so the string values need to be converted into lists of numbers. To do that, map each unique word to a unique integer.\n",
    "\n",
    "### Build vocabulary\n",
    "\n",
    "First, build a vocabulary by tokenizing the text into a collection of individual unique words. There are a few ways to do this in both TensorFlow and Python. For this tutorial:\n",
    "\n",
    "1. Iterate over each example's `numpy` value.\n",
    "2. Use `tfds.features.text.Tokenizer` to split it into tokens.\n",
    "3. Collect these tokens into a Python set, to remove duplicates.\n",
    "4. Get the size of the vocabulary for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17178"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "# 유니크한 token update\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode examples\n",
    "\n",
    "Create an encoder by passing the `vocabulary_set` to `tfds.features.text.TokenTextEncoder`. The encoder's `encode` method takes in a string of text and returns a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'It was when they were doing the last part of the course on their way'\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6205, 13844, 2444, 528, 13878, 2861, 16294, 16660, 14568, 923, 16294, 14306, 889, 5258, 7986]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example) # 단어 Token 번호로 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "# python 사용자 정의함수를 사용할수있게 해주는ㄴ tf.py_function\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE) # skip skip 하는만큼\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE) # take는 take 하는만큼 가지고옴\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "# padded_batch는 각 문서마다 길이 다른걸 고려한 거임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=99562, shape=(16,), dtype=int64, numpy=\n",
       " array([ 6205, 13844,  2444,   528, 13878,  2861, 16294, 16660, 14568,\n",
       "          923, 16294, 14306,   889,  5258,  7986,     0], dtype=int64)>,\n",
       " <tf.Tensor: id=99566, shape=(), dtype=int64, numpy=2>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1 # for zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "model.add(tf.keras.layers.Embedding(vocab_size,embedding_dim))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, None, 100)         1717900   \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,814,991\n",
      "Trainable params: 1,814,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    168/Unknown - 16s 16s/step - loss: 1.0998 - accuracy: 0.29 - 16s 8s/step - loss: 1.0954 - accuracy: 0.3438 - 16s 5s/step - loss: 1.0907 - accuracy: 0.354 - 16s 4s/step - loss: 1.0893 - accuracy: 0.335 - 16s 3s/step - loss: 1.0857 - accuracy: 0.337 - 16s 3s/step - loss: 1.0820 - accuracy: 0.333 - 16s 2s/step - loss: 1.0799 - accuracy: 0.339 - 16s 2s/step - loss: 1.0746 - accuracy: 0.353 - 16s 2s/step - loss: 1.0721 - accuracy: 0.357 - 16s 2s/step - loss: 1.0696 - accuracy: 0.354 - 17s 2s/step - loss: 1.0657 - accuracy: 0.353 - 17s 1s/step - loss: 1.0605 - accuracy: 0.367 - 17s 1s/step - loss: 1.0601 - accuracy: 0.366 - 17s 1s/step - loss: 1.0580 - accuracy: 0.371 - 17s 1s/step - loss: 1.0511 - accuracy: 0.379 - 17s 1s/step - loss: 1.0445 - accuracy: 0.389 - 17s 988ms/step - loss: 1.0371 - accuracy: 0.39 - 17s 936ms/step - loss: 1.0336 - accuracy: 0.39 - 17s 889ms/step - loss: 1.0294 - accuracy: 0.40 - 17s 847ms/step - loss: 1.0201 - accuracy: 0.41 - 17s 808ms/step - loss: 1.0099 - accuracy: 0.41 - 17s 774ms/step - loss: 1.0051 - accuracy: 0.42 - 17s 743ms/step - loss: 0.9955 - accuracy: 0.42 - 17s 714ms/step - loss: 0.9870 - accuracy: 0.43 - 17s 687ms/step - loss: 0.9778 - accuracy: 0.43 - 17s 662ms/step - loss: 0.9730 - accuracy: 0.44 - 17s 639ms/step - loss: 0.9636 - accuracy: 0.45 - 17s 618ms/step - loss: 0.9543 - accuracy: 0.46 - 17s 598ms/step - loss: 0.9431 - accuracy: 0.47 - 17s 580ms/step - loss: 0.9424 - accuracy: 0.48 - 17s 562ms/step - loss: 0.9347 - accuracy: 0.48 - 17s 546ms/step - loss: 0.9247 - accuracy: 0.49 - 18s 531ms/step - loss: 0.9171 - accuracy: 0.49 - 18s 517ms/step - loss: 0.9091 - accuracy: 0.49 - 18s 503ms/step - loss: 0.9051 - accuracy: 0.50 - 18s 490ms/step - loss: 0.8999 - accuracy: 0.49 - 18s 478ms/step - loss: 0.8986 - accuracy: 0.50 - 18s 467ms/step - loss: 0.8897 - accuracy: 0.50 - 18s 456ms/step - loss: 0.8853 - accuracy: 0.50 - 18s 446ms/step - loss: 0.8822 - accuracy: 0.51 - 18s 436ms/step - loss: 0.8770 - accuracy: 0.51 - 18s 427ms/step - loss: 0.8767 - accuracy: 0.51 - 18s 418ms/step - loss: 0.8764 - accuracy: 0.51 - 18s 409ms/step - loss: 0.8715 - accuracy: 0.51 - 18s 401ms/step - loss: 0.8677 - accuracy: 0.51 - 18s 393ms/step - loss: 0.8655 - accuracy: 0.52 - 18s 386ms/step - loss: 0.8609 - accuracy: 0.52 - 18s 378ms/step - loss: 0.8572 - accuracy: 0.52 - 18s 372ms/step - loss: 0.8522 - accuracy: 0.52 - 18s 365ms/step - loss: 0.8490 - accuracy: 0.52 - 18s 359ms/step - loss: 0.8432 - accuracy: 0.53 - 18s 353ms/step - loss: 0.8385 - accuracy: 0.53 - 18s 347ms/step - loss: 0.8362 - accuracy: 0.53 - 18s 341ms/step - loss: 0.8332 - accuracy: 0.53 - 18s 336ms/step - loss: 0.8321 - accuracy: 0.53 - 19s 331ms/step - loss: 0.8284 - accuracy: 0.54 - 19s 326ms/step - loss: 0.8272 - accuracy: 0.54 - 19s 321ms/step - loss: 0.8251 - accuracy: 0.54 - 19s 316ms/step - loss: 0.8220 - accuracy: 0.54 - 19s 312ms/step - loss: 0.8183 - accuracy: 0.54 - 19s 307ms/step - loss: 0.8165 - accuracy: 0.54 - 19s 303ms/step - loss: 0.8133 - accuracy: 0.54 - 19s 299ms/step - loss: 0.8104 - accuracy: 0.54 - 19s 295ms/step - loss: 0.8089 - accuracy: 0.55 - 19s 291ms/step - loss: 0.8056 - accuracy: 0.55 - 19s 287ms/step - loss: 0.8030 - accuracy: 0.55 - 19s 284ms/step - loss: 0.7982 - accuracy: 0.55 - 19s 280ms/step - loss: 0.7981 - accuracy: 0.55 - 19s 277ms/step - loss: 0.7951 - accuracy: 0.55 - 19s 273ms/step - loss: 0.7932 - accuracy: 0.55 - 19s 270ms/step - loss: 0.7912 - accuracy: 0.56 - 19s 267ms/step - loss: 0.7881 - accuracy: 0.56 - 19s 264ms/step - loss: 0.7859 - accuracy: 0.56 - 19s 261ms/step - loss: 0.7857 - accuracy: 0.56 - 19s 258ms/step - loss: 0.7843 - accuracy: 0.56 - 19s 255ms/step - loss: 0.7825 - accuracy: 0.56 - 19s 253ms/step - loss: 0.7807 - accuracy: 0.56 - 20s 250ms/step - loss: 0.7816 - accuracy: 0.56 - 20s 248ms/step - loss: 0.7794 - accuracy: 0.56 - 20s 245ms/step - loss: 0.7774 - accuracy: 0.56 - 20s 243ms/step - loss: 0.7767 - accuracy: 0.56 - 20s 240ms/step - loss: 0.7757 - accuracy: 0.56 - 20s 238ms/step - loss: 0.7726 - accuracy: 0.57 - 20s 236ms/step - loss: 0.7716 - accuracy: 0.57 - 20s 234ms/step - loss: 0.7687 - accuracy: 0.57 - 20s 231ms/step - loss: 0.7671 - accuracy: 0.57 - 20s 229ms/step - loss: 0.7658 - accuracy: 0.57 - 20s 227ms/step - loss: 0.7633 - accuracy: 0.57 - 20s 225ms/step - loss: 0.7622 - accuracy: 0.57 - 20s 223ms/step - loss: 0.7622 - accuracy: 0.57 - 20s 221ms/step - loss: 0.7623 - accuracy: 0.57 - 20s 219ms/step - loss: 0.7600 - accuracy: 0.57 - 20s 218ms/step - loss: 0.7588 - accuracy: 0.57 - 20s 216ms/step - loss: 0.7565 - accuracy: 0.57 - 20s 214ms/step - loss: 0.7548 - accuracy: 0.58 - 20s 212ms/step - loss: 0.7525 - accuracy: 0.58 - 20s 210ms/step - loss: 0.7518 - accuracy: 0.58 - 20s 209ms/step - loss: 0.7508 - accuracy: 0.58 - 21s 207ms/step - loss: 0.7508 - accuracy: 0.58 - 21s 206ms/step - loss: 0.7494 - accuracy: 0.58 - 21s 204ms/step - loss: 0.7476 - accuracy: 0.58 - 21s 202ms/step - loss: 0.7463 - accuracy: 0.58 - 21s 201ms/step - loss: 0.7449 - accuracy: 0.58 - 21s 199ms/step - loss: 0.7439 - accuracy: 0.58 - 21s 198ms/step - loss: 0.7433 - accuracy: 0.58 - 21s 196ms/step - loss: 0.7420 - accuracy: 0.58 - 21s 195ms/step - loss: 0.7404 - accuracy: 0.58 - 21s 194ms/step - loss: 0.7387 - accuracy: 0.58 - 21s 192ms/step - loss: 0.7379 - accuracy: 0.58 - 21s 191ms/step - loss: 0.7359 - accuracy: 0.59 - 21s 190ms/step - loss: 0.7338 - accuracy: 0.59 - 21s 188ms/step - loss: 0.7322 - accuracy: 0.59 - 21s 187ms/step - loss: 0.7313 - accuracy: 0.59 - 21s 186ms/step - loss: 0.7317 - accuracy: 0.59 - 21s 184ms/step - loss: 0.7304 - accuracy: 0.59 - 21s 183ms/step - loss: 0.7296 - accuracy: 0.59 - 21s 182ms/step - loss: 0.7290 - accuracy: 0.59 - 21s 181ms/step - loss: 0.7289 - accuracy: 0.59 - 21s 180ms/step - loss: 0.7277 - accuracy: 0.59 - 21s 179ms/step - loss: 0.7267 - accuracy: 0.59 - 21s 178ms/step - loss: 0.7249 - accuracy: 0.59 - 22s 177ms/step - loss: 0.7234 - accuracy: 0.59 - 22s 176ms/step - loss: 0.7228 - accuracy: 0.59 - 22s 174ms/step - loss: 0.7222 - accuracy: 0.59 - 22s 173ms/step - loss: 0.7226 - accuracy: 0.59 - 22s 172ms/step - loss: 0.7233 - accuracy: 0.59 - 22s 171ms/step - loss: 0.7226 - accuracy: 0.59 - 22s 170ms/step - loss: 0.7220 - accuracy: 0.60 - 22s 169ms/step - loss: 0.7217 - accuracy: 0.60 - 22s 168ms/step - loss: 0.7204 - accuracy: 0.60 - 22s 167ms/step - loss: 0.7195 - accuracy: 0.60 - 22s 166ms/step - loss: 0.7180 - accuracy: 0.60 - 22s 165ms/step - loss: 0.7171 - accuracy: 0.60 - 22s 165ms/step - loss: 0.7161 - accuracy: 0.60 - 22s 164ms/step - loss: 0.7152 - accuracy: 0.60 - 22s 163ms/step - loss: 0.7149 - accuracy: 0.60 - 22s 162ms/step - loss: 0.7141 - accuracy: 0.60 - 22s 161ms/step - loss: 0.7130 - accuracy: 0.60 - 22s 160ms/step - loss: 0.7116 - accuracy: 0.60 - 22s 159ms/step - loss: 0.7106 - accuracy: 0.60 - 22s 159ms/step - loss: 0.7097 - accuracy: 0.60 - 22s 158ms/step - loss: 0.7087 - accuracy: 0.60 - 22s 157ms/step - loss: 0.7082 - accuracy: 0.60 - 22s 156ms/step - loss: 0.7075 - accuracy: 0.60 - 23s 155ms/step - loss: 0.7068 - accuracy: 0.61 - 23s 155ms/step - loss: 0.7058 - accuracy: 0.61 - 23s 154ms/step - loss: 0.7052 - accuracy: 0.61 - 23s 153ms/step - loss: 0.7049 - accuracy: 0.61 - 23s 152ms/step - loss: 0.7032 - accuracy: 0.61 - 23s 152ms/step - loss: 0.7017 - accuracy: 0.61 - 23s 151ms/step - loss: 0.7011 - accuracy: 0.61 - 23s 150ms/step - loss: 0.7005 - accuracy: 0.61 - 23s 150ms/step - loss: 0.7008 - accuracy: 0.61 - 23s 149ms/step - loss: 0.6999 - accuracy: 0.61 - 23s 148ms/step - loss: 0.6995 - accuracy: 0.61 - 23s 148ms/step - loss: 0.6985 - accuracy: 0.61 - 23s 147ms/step - loss: 0.6975 - accuracy: 0.61 - 23s 146ms/step - loss: 0.6967 - accuracy: 0.61 - 23s 146ms/step - loss: 0.6960 - accuracy: 0.61 - 23s 145ms/step - loss: 0.6949 - accuracy: 0.61 - 23s 144ms/step - loss: 0.6940 - accuracy: 0.62 - 23s 144ms/step - loss: 0.6934 - accuracy: 0.62 - 23s 143ms/step - loss: 0.6921 - accuracy: 0.62 - 23s 142ms/step - loss: 0.6907 - accuracy: 0.62 - 23s 142ms/step - loss: 0.6893 - accuracy: 0.62 - 23s 141ms/step - loss: 0.6883 - accuracy: 0.62 - 24s 141ms/step - loss: 0.6873 - accuracy: 0.62 - 24s 140ms/step - loss: 0.6858 - accuracy: 0.62    335/Unknown - 24s 140ms/step - loss: 0.6853 - accuracy: 0.62 - 24s 139ms/step - loss: 0.6846 - accuracy: 0.62 - 24s 139ms/step - loss: 0.6848 - accuracy: 0.62 - 24s 138ms/step - loss: 0.6843 - accuracy: 0.62 - 24s 137ms/step - loss: 0.6838 - accuracy: 0.62 - 24s 137ms/step - loss: 0.6836 - accuracy: 0.62 - 24s 136ms/step - loss: 0.6835 - accuracy: 0.62 - 24s 136ms/step - loss: 0.6826 - accuracy: 0.62 - 24s 135ms/step - loss: 0.6814 - accuracy: 0.62 - 24s 135ms/step - loss: 0.6815 - accuracy: 0.62 - 24s 134ms/step - loss: 0.6812 - accuracy: 0.63 - 24s 134ms/step - loss: 0.6806 - accuracy: 0.63 - 24s 133ms/step - loss: 0.6799 - accuracy: 0.63 - 24s 133ms/step - loss: 0.6794 - accuracy: 0.63 - 24s 132ms/step - loss: 0.6789 - accuracy: 0.63 - 24s 132ms/step - loss: 0.6779 - accuracy: 0.63 - 24s 131ms/step - loss: 0.6775 - accuracy: 0.63 - 24s 131ms/step - loss: 0.6764 - accuracy: 0.63 - 24s 130ms/step - loss: 0.6753 - accuracy: 0.63 - 24s 130ms/step - loss: 0.6752 - accuracy: 0.63 - 24s 130ms/step - loss: 0.6750 - accuracy: 0.63 - 25s 129ms/step - loss: 0.6737 - accuracy: 0.63 - 25s 129ms/step - loss: 0.6732 - accuracy: 0.63 - 25s 128ms/step - loss: 0.6730 - accuracy: 0.63 - 25s 128ms/step - loss: 0.6724 - accuracy: 0.63 - 25s 127ms/step - loss: 0.6711 - accuracy: 0.63 - 25s 127ms/step - loss: 0.6704 - accuracy: 0.63 - 25s 127ms/step - loss: 0.6698 - accuracy: 0.64 - 25s 126ms/step - loss: 0.6690 - accuracy: 0.64 - 25s 126ms/step - loss: 0.6679 - accuracy: 0.64 - 25s 125ms/step - loss: 0.6674 - accuracy: 0.64 - 25s 125ms/step - loss: 0.6669 - accuracy: 0.64 - 25s 125ms/step - loss: 0.6662 - accuracy: 0.64 - 25s 124ms/step - loss: 0.6651 - accuracy: 0.64 - 25s 124ms/step - loss: 0.6645 - accuracy: 0.64 - 25s 123ms/step - loss: 0.6643 - accuracy: 0.64 - 25s 123ms/step - loss: 0.6638 - accuracy: 0.64 - 25s 123ms/step - loss: 0.6636 - accuracy: 0.64 - 25s 122ms/step - loss: 0.6628 - accuracy: 0.64 - 25s 122ms/step - loss: 0.6615 - accuracy: 0.64 - 25s 121ms/step - loss: 0.6611 - accuracy: 0.64 - 25s 121ms/step - loss: 0.6608 - accuracy: 0.64 - 25s 121ms/step - loss: 0.6600 - accuracy: 0.64 - 26s 120ms/step - loss: 0.6589 - accuracy: 0.64 - 26s 120ms/step - loss: 0.6582 - accuracy: 0.65 - 26s 120ms/step - loss: 0.6580 - accuracy: 0.65 - 26s 119ms/step - loss: 0.6572 - accuracy: 0.65 - 26s 119ms/step - loss: 0.6561 - accuracy: 0.65 - 26s 119ms/step - loss: 0.6554 - accuracy: 0.65 - 26s 118ms/step - loss: 0.6554 - accuracy: 0.65 - 26s 118ms/step - loss: 0.6548 - accuracy: 0.65 - 26s 118ms/step - loss: 0.6541 - accuracy: 0.65 - 26s 117ms/step - loss: 0.6530 - accuracy: 0.65 - 26s 117ms/step - loss: 0.6522 - accuracy: 0.65 - 26s 117ms/step - loss: 0.6510 - accuracy: 0.65 - 26s 116ms/step - loss: 0.6503 - accuracy: 0.65 - 26s 116ms/step - loss: 0.6498 - accuracy: 0.65 - 26s 116ms/step - loss: 0.6496 - accuracy: 0.65 - 26s 116ms/step - loss: 0.6489 - accuracy: 0.65 - 26s 115ms/step - loss: 0.6477 - accuracy: 0.65 - 26s 115ms/step - loss: 0.6472 - accuracy: 0.65 - 26s 115ms/step - loss: 0.6465 - accuracy: 0.65 - 26s 114ms/step - loss: 0.6463 - accuracy: 0.65 - 26s 114ms/step - loss: 0.6458 - accuracy: 0.66 - 27s 114ms/step - loss: 0.6450 - accuracy: 0.66 - 27s 114ms/step - loss: 0.6445 - accuracy: 0.66 - 27s 113ms/step - loss: 0.6441 - accuracy: 0.66 - 27s 113ms/step - loss: 0.6434 - accuracy: 0.66 - 27s 113ms/step - loss: 0.6434 - accuracy: 0.66 - 27s 113ms/step - loss: 0.6432 - accuracy: 0.66 - 27s 112ms/step - loss: 0.6426 - accuracy: 0.66 - 27s 112ms/step - loss: 0.6422 - accuracy: 0.66 - 27s 112ms/step - loss: 0.6419 - accuracy: 0.66 - 27s 111ms/step - loss: 0.6414 - accuracy: 0.66 - 27s 111ms/step - loss: 0.6408 - accuracy: 0.66 - 27s 111ms/step - loss: 0.6402 - accuracy: 0.66 - 27s 111ms/step - loss: 0.6397 - accuracy: 0.66 - 27s 111ms/step - loss: 0.6388 - accuracy: 0.66 - 27s 110ms/step - loss: 0.6381 - accuracy: 0.66 - 27s 110ms/step - loss: 0.6378 - accuracy: 0.66 - 27s 110ms/step - loss: 0.6372 - accuracy: 0.66 - 27s 109ms/step - loss: 0.6370 - accuracy: 0.66 - 27s 109ms/step - loss: 0.6368 - accuracy: 0.66 - 27s 109ms/step - loss: 0.6363 - accuracy: 0.66 - 28s 109ms/step - loss: 0.6356 - accuracy: 0.66 - 28s 109ms/step - loss: 0.6347 - accuracy: 0.66 - 28s 108ms/step - loss: 0.6341 - accuracy: 0.67 - 28s 108ms/step - loss: 0.6333 - accuracy: 0.67 - 28s 108ms/step - loss: 0.6325 - accuracy: 0.67 - 28s 108ms/step - loss: 0.6319 - accuracy: 0.67 - 28s 107ms/step - loss: 0.6312 - accuracy: 0.67 - 28s 107ms/step - loss: 0.6308 - accuracy: 0.67 - 28s 107ms/step - loss: 0.6306 - accuracy: 0.67 - 28s 107ms/step - loss: 0.6299 - accuracy: 0.67 - 28s 106ms/step - loss: 0.6292 - accuracy: 0.67 - 28s 106ms/step - loss: 0.6286 - accuracy: 0.67 - 28s 106ms/step - loss: 0.6286 - accuracy: 0.67 - 28s 106ms/step - loss: 0.6280 - accuracy: 0.67 - 28s 105ms/step - loss: 0.6273 - accuracy: 0.67 - 28s 105ms/step - loss: 0.6262 - accuracy: 0.67 - 28s 105ms/step - loss: 0.6253 - accuracy: 0.67 - 28s 105ms/step - loss: 0.6248 - accuracy: 0.67 - 28s 104ms/step - loss: 0.6246 - accuracy: 0.67 - 28s 104ms/step - loss: 0.6239 - accuracy: 0.67 - 28s 104ms/step - loss: 0.6232 - accuracy: 0.67 - 28s 104ms/step - loss: 0.6236 - accuracy: 0.67 - 28s 104ms/step - loss: 0.6226 - accuracy: 0.67 - 29s 103ms/step - loss: 0.6216 - accuracy: 0.67 - 29s 103ms/step - loss: 0.6214 - accuracy: 0.68 - 29s 103ms/step - loss: 0.6204 - accuracy: 0.68 - 29s 103ms/step - loss: 0.6196 - accuracy: 0.68 - 29s 103ms/step - loss: 0.6188 - accuracy: 0.68 - 29s 102ms/step - loss: 0.6180 - accuracy: 0.68 - 29s 102ms/step - loss: 0.6175 - accuracy: 0.68 - 29s 102ms/step - loss: 0.6177 - accuracy: 0.68 - 29s 102ms/step - loss: 0.6176 - accuracy: 0.68 - 29s 102ms/step - loss: 0.6171 - accuracy: 0.68 - 29s 101ms/step - loss: 0.6167 - accuracy: 0.68 - 29s 101ms/step - loss: 0.6164 - accuracy: 0.68 - 29s 101ms/step - loss: 0.6160 - accuracy: 0.68 - 29s 101ms/step - loss: 0.6155 - accuracy: 0.68 - 29s 101ms/step - loss: 0.6149 - accuracy: 0.68 - 29s 100ms/step - loss: 0.6144 - accuracy: 0.68 - 29s 100ms/step - loss: 0.6137 - accuracy: 0.68 - 29s 100ms/step - loss: 0.6136 - accuracy: 0.68 - 29s 100ms/step - loss: 0.6130 - accuracy: 0.68 - 29s 100ms/step - loss: 0.6123 - accuracy: 0.68 - 29s 99ms/step - loss: 0.6123 - accuracy: 0.6861 - 29s 99ms/step - loss: 0.6124 - accuracy: 0.686 - 30s 99ms/step - loss: 0.6117 - accuracy: 0.686 - 30s 99ms/step - loss: 0.6116 - accuracy: 0.686 - 30s 99ms/step - loss: 0.6112 - accuracy: 0.687 - 30s 99ms/step - loss: 0.6112 - accuracy: 0.687 - 30s 98ms/step - loss: 0.6106 - accuracy: 0.687 - 30s 98ms/step - loss: 0.6107 - accuracy: 0.687 - 30s 98ms/step - loss: 0.6106 - accuracy: 0.688 - 30s 98ms/step - loss: 0.6106 - accuracy: 0.688 - 30s 98ms/step - loss: 0.6100 - accuracy: 0.688 - 30s 97ms/step - loss: 0.6096 - accuracy: 0.688 - 30s 97ms/step - loss: 0.6090 - accuracy: 0.689 - 30s 97ms/step - loss: 0.6089 - accuracy: 0.689 - 30s 97ms/step - loss: 0.6083 - accuracy: 0.689 - 30s 97ms/step - loss: 0.6080 - accuracy: 0.690 - 30s 97ms/step - loss: 0.6075 - accuracy: 0.690 - 30s 96ms/step - loss: 0.6070 - accuracy: 0.690 - 30s 96ms/step - loss: 0.6065 - accuracy: 0.691 - 30s 96ms/step - loss: 0.6064 - accuracy: 0.691 - 30s 96ms/step - loss: 0.6058 - accuracy: 0.691 - 30s 96ms/step - loss: 0.6056 - accuracy: 0.691 - 30s 96ms/step - loss: 0.6058 - accuracy: 0.692 - 30s 95ms/step - loss: 0.6054 - accuracy: 0.692 - 30s 95ms/step - loss: 0.6047 - accuracy: 0.692 - 31s 95ms/step - loss: 0.6045 - accuracy: 0.692 - 31s 95ms/step - loss: 0.6040 - accuracy: 0.693 - 31s 95ms/step - loss: 0.6035 - accuracy: 0.693 - 31s 95ms/step - loss: 0.6030 - accuracy: 0.693 - 31s 94ms/step - loss: 0.6028 - accuracy: 0.693 - 31s 94ms/step - loss: 0.6025 - accuracy: 0.694 - 31s 94ms/step - loss: 0.6023 - accuracy: 0.694 - 31s 94ms/step - loss: 0.6016 - accuracy: 0.694 - 31s 94ms/step - loss: 0.6011 - accuracy: 0.695 - 31s 94ms/step - loss: 0.6006 - accuracy: 0.695 - 31s 94ms/step - loss: 0.6001 - accuracy: 0.696 - 31s 93ms/step - loss: 0.5996 - accuracy: 0.696 - 31s 93ms/step - loss: 0.5996 - accuracy: 0.696 - 31s 93ms/step - loss: 0.5992 - accuracy: 0.696 - 31s 93ms/step - loss: 0.5986 - accuracy: 0.6971"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    502/Unknown - 31s 93ms/step - loss: 0.5980 - accuracy: 0.697 - 31s 93ms/step - loss: 0.5980 - accuracy: 0.697 - 31s 93ms/step - loss: 0.5975 - accuracy: 0.697 - 31s 92ms/step - loss: 0.5969 - accuracy: 0.698 - 31s 92ms/step - loss: 0.5966 - accuracy: 0.698 - 31s 92ms/step - loss: 0.5959 - accuracy: 0.699 - 31s 92ms/step - loss: 0.5954 - accuracy: 0.699 - 31s 92ms/step - loss: 0.5952 - accuracy: 0.699 - 32s 92ms/step - loss: 0.5946 - accuracy: 0.699 - 32s 91ms/step - loss: 0.5944 - accuracy: 0.700 - 32s 91ms/step - loss: 0.5940 - accuracy: 0.700 - 32s 91ms/step - loss: 0.5936 - accuracy: 0.700 - 32s 91ms/step - loss: 0.5935 - accuracy: 0.700 - 32s 91ms/step - loss: 0.5931 - accuracy: 0.700 - 32s 91ms/step - loss: 0.5928 - accuracy: 0.701 - 32s 91ms/step - loss: 0.5923 - accuracy: 0.701 - 32s 91ms/step - loss: 0.5919 - accuracy: 0.701 - 32s 90ms/step - loss: 0.5915 - accuracy: 0.702 - 32s 90ms/step - loss: 0.5915 - accuracy: 0.702 - 32s 90ms/step - loss: 0.5914 - accuracy: 0.702 - 32s 90ms/step - loss: 0.5915 - accuracy: 0.702 - 32s 90ms/step - loss: 0.5913 - accuracy: 0.702 - 32s 90ms/step - loss: 0.5908 - accuracy: 0.702 - 32s 90ms/step - loss: 0.5907 - accuracy: 0.703 - 32s 90ms/step - loss: 0.5903 - accuracy: 0.703 - 32s 89ms/step - loss: 0.5894 - accuracy: 0.703 - 32s 89ms/step - loss: 0.5887 - accuracy: 0.704 - 32s 89ms/step - loss: 0.5883 - accuracy: 0.704 - 32s 89ms/step - loss: 0.5878 - accuracy: 0.704 - 32s 89ms/step - loss: 0.5874 - accuracy: 0.705 - 33s 89ms/step - loss: 0.5868 - accuracy: 0.705 - 33s 89ms/step - loss: 0.5863 - accuracy: 0.705 - 33s 89ms/step - loss: 0.5860 - accuracy: 0.705 - 33s 89ms/step - loss: 0.5855 - accuracy: 0.706 - 33s 88ms/step - loss: 0.5848 - accuracy: 0.706 - 33s 88ms/step - loss: 0.5842 - accuracy: 0.707 - 33s 88ms/step - loss: 0.5839 - accuracy: 0.707 - 33s 88ms/step - loss: 0.5841 - accuracy: 0.707 - 33s 88ms/step - loss: 0.5837 - accuracy: 0.707 - 33s 88ms/step - loss: 0.5836 - accuracy: 0.707 - 33s 88ms/step - loss: 0.5834 - accuracy: 0.707 - 33s 88ms/step - loss: 0.5827 - accuracy: 0.708 - 33s 88ms/step - loss: 0.5821 - accuracy: 0.708 - 33s 88ms/step - loss: 0.5816 - accuracy: 0.708 - 33s 87ms/step - loss: 0.5810 - accuracy: 0.709 - 33s 87ms/step - loss: 0.5804 - accuracy: 0.709 - 33s 87ms/step - loss: 0.5801 - accuracy: 0.709 - 33s 87ms/step - loss: 0.5799 - accuracy: 0.710 - 33s 87ms/step - loss: 0.5791 - accuracy: 0.710 - 33s 87ms/step - loss: 0.5787 - accuracy: 0.710 - 34s 87ms/step - loss: 0.5782 - accuracy: 0.710 - 34s 87ms/step - loss: 0.5776 - accuracy: 0.711 - 34s 87ms/step - loss: 0.5774 - accuracy: 0.711 - 34s 87ms/step - loss: 0.5770 - accuracy: 0.711 - 34s 86ms/step - loss: 0.5768 - accuracy: 0.711 - 34s 86ms/step - loss: 0.5765 - accuracy: 0.711 - 34s 86ms/step - loss: 0.5761 - accuracy: 0.712 - 34s 86ms/step - loss: 0.5757 - accuracy: 0.712 - 34s 86ms/step - loss: 0.5754 - accuracy: 0.712 - 34s 86ms/step - loss: 0.5751 - accuracy: 0.712 - 34s 86ms/step - loss: 0.5750 - accuracy: 0.713 - 34s 86ms/step - loss: 0.5743 - accuracy: 0.713 - 34s 86ms/step - loss: 0.5739 - accuracy: 0.713 - 34s 86ms/step - loss: 0.5735 - accuracy: 0.713 - 34s 85ms/step - loss: 0.5730 - accuracy: 0.714 - 34s 85ms/step - loss: 0.5724 - accuracy: 0.714 - 34s 85ms/step - loss: 0.5720 - accuracy: 0.715 - 34s 85ms/step - loss: 0.5719 - accuracy: 0.715 - 34s 85ms/step - loss: 0.5715 - accuracy: 0.715 - 34s 85ms/step - loss: 0.5707 - accuracy: 0.715 - 34s 85ms/step - loss: 0.5704 - accuracy: 0.715 - 34s 85ms/step - loss: 0.5701 - accuracy: 0.716 - 35s 85ms/step - loss: 0.5698 - accuracy: 0.716 - 35s 85ms/step - loss: 0.5692 - accuracy: 0.716 - 35s 84ms/step - loss: 0.5688 - accuracy: 0.716 - 35s 84ms/step - loss: 0.5682 - accuracy: 0.717 - 35s 84ms/step - loss: 0.5682 - accuracy: 0.717 - 35s 84ms/step - loss: 0.5681 - accuracy: 0.717 - 35s 84ms/step - loss: 0.5679 - accuracy: 0.717 - 35s 84ms/step - loss: 0.5674 - accuracy: 0.718 - 35s 84ms/step - loss: 0.5672 - accuracy: 0.718 - 35s 84ms/step - loss: 0.5670 - accuracy: 0.718 - 35s 84ms/step - loss: 0.5666 - accuracy: 0.718 - 35s 83ms/step - loss: 0.5664 - accuracy: 0.718 - 35s 83ms/step - loss: 0.5663 - accuracy: 0.718 - 35s 83ms/step - loss: 0.5658 - accuracy: 0.719 - 35s 83ms/step - loss: 0.5656 - accuracy: 0.719 - 35s 83ms/step - loss: 0.5651 - accuracy: 0.719 - 35s 83ms/step - loss: 0.5651 - accuracy: 0.719 - 35s 83ms/step - loss: 0.5646 - accuracy: 0.719 - 35s 83ms/step - loss: 0.5645 - accuracy: 0.720 - 35s 83ms/step - loss: 0.5642 - accuracy: 0.720 - 35s 83ms/step - loss: 0.5636 - accuracy: 0.720 - 35s 83ms/step - loss: 0.5633 - accuracy: 0.720 - 35s 82ms/step - loss: 0.5631 - accuracy: 0.720 - 35s 82ms/step - loss: 0.5627 - accuracy: 0.721 - 36s 82ms/step - loss: 0.5625 - accuracy: 0.721 - 36s 82ms/step - loss: 0.5622 - accuracy: 0.721 - 36s 82ms/step - loss: 0.5617 - accuracy: 0.721 - 36s 82ms/step - loss: 0.5614 - accuracy: 0.721 - 36s 82ms/step - loss: 0.5612 - accuracy: 0.721 - 36s 82ms/step - loss: 0.5610 - accuracy: 0.722 - 36s 82ms/step - loss: 0.5609 - accuracy: 0.722 - 36s 82ms/step - loss: 0.5608 - accuracy: 0.722 - 36s 82ms/step - loss: 0.5610 - accuracy: 0.722 - 36s 82ms/step - loss: 0.5606 - accuracy: 0.722 - 36s 82ms/step - loss: 0.5606 - accuracy: 0.722 - 36s 81ms/step - loss: 0.5603 - accuracy: 0.722 - 36s 81ms/step - loss: 0.5600 - accuracy: 0.722 - 36s 81ms/step - loss: 0.5598 - accuracy: 0.723 - 36s 81ms/step - loss: 0.5600 - accuracy: 0.723 - 36s 81ms/step - loss: 0.5598 - accuracy: 0.723 - 36s 81ms/step - loss: 0.5596 - accuracy: 0.723 - 36s 81ms/step - loss: 0.5593 - accuracy: 0.723 - 36s 81ms/step - loss: 0.5591 - accuracy: 0.723 - 37s 81ms/step - loss: 0.5588 - accuracy: 0.723 - 37s 81ms/step - loss: 0.5584 - accuracy: 0.724 - 37s 81ms/step - loss: 0.5581 - accuracy: 0.724 - 37s 81ms/step - loss: 0.5575 - accuracy: 0.724 - 37s 81ms/step - loss: 0.5572 - accuracy: 0.724 - 37s 81ms/step - loss: 0.5572 - accuracy: 0.725 - 37s 81ms/step - loss: 0.5568 - accuracy: 0.725 - 37s 81ms/step - loss: 0.5566 - accuracy: 0.725 - 37s 81ms/step - loss: 0.5564 - accuracy: 0.725 - 37s 80ms/step - loss: 0.5562 - accuracy: 0.725 - 37s 80ms/step - loss: 0.5557 - accuracy: 0.726 - 37s 80ms/step - loss: 0.5553 - accuracy: 0.726 - 37s 80ms/step - loss: 0.5548 - accuracy: 0.726 - 37s 80ms/step - loss: 0.5545 - accuracy: 0.726 - 37s 80ms/step - loss: 0.5541 - accuracy: 0.726 - 37s 80ms/step - loss: 0.5537 - accuracy: 0.727 - 37s 80ms/step - loss: 0.5532 - accuracy: 0.727 - 37s 80ms/step - loss: 0.5530 - accuracy: 0.727 - 37s 80ms/step - loss: 0.5526 - accuracy: 0.727 - 37s 80ms/step - loss: 0.5519 - accuracy: 0.728 - 38s 80ms/step - loss: 0.5516 - accuracy: 0.728 - 38s 80ms/step - loss: 0.5511 - accuracy: 0.728 - 38s 80ms/step - loss: 0.5508 - accuracy: 0.728 - 38s 79ms/step - loss: 0.5504 - accuracy: 0.729 - 38s 79ms/step - loss: 0.5500 - accuracy: 0.729 - 38s 79ms/step - loss: 0.5496 - accuracy: 0.729 - 38s 79ms/step - loss: 0.5492 - accuracy: 0.730 - 38s 79ms/step - loss: 0.5487 - accuracy: 0.730 - 38s 79ms/step - loss: 0.5483 - accuracy: 0.730 - 38s 79ms/step - loss: 0.5480 - accuracy: 0.730 - 38s 79ms/step - loss: 0.5479 - accuracy: 0.730 - 38s 79ms/step - loss: 0.5475 - accuracy: 0.731 - 38s 79ms/step - loss: 0.5474 - accuracy: 0.731 - 38s 79ms/step - loss: 0.5471 - accuracy: 0.731 - 38s 79ms/step - loss: 0.5470 - accuracy: 0.731 - 38s 79ms/step - loss: 0.5466 - accuracy: 0.731 - 38s 78ms/step - loss: 0.5463 - accuracy: 0.731 - 38s 78ms/step - loss: 0.5462 - accuracy: 0.731 - 38s 78ms/step - loss: 0.5461 - accuracy: 0.731 - 38s 78ms/step - loss: 0.5456 - accuracy: 0.732 - 38s 78ms/step - loss: 0.5457 - accuracy: 0.732 - 38s 78ms/step - loss: 0.5452 - accuracy: 0.732 - 38s 78ms/step - loss: 0.5449 - accuracy: 0.732 - 39s 78ms/step - loss: 0.5448 - accuracy: 0.732 - 39s 78ms/step - loss: 0.5444 - accuracy: 0.732 - 39s 78ms/step - loss: 0.5442 - accuracy: 0.732 - 39s 78ms/step - loss: 0.5438 - accuracy: 0.733 - 39s 78ms/step - loss: 0.5435 - accuracy: 0.733 - 39s 78ms/step - loss: 0.5430 - accuracy: 0.733 - 39s 78ms/step - loss: 0.5429 - accuracy: 0.733 - 39s 78ms/step - loss: 0.5427 - accuracy: 0.733 - 39s 77ms/step - loss: 0.5424 - accuracy: 0.7339"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    669/Unknown - 39s 77ms/step - loss: 0.5420 - accuracy: 0.734 - 39s 77ms/step - loss: 0.5417 - accuracy: 0.734 - 39s 77ms/step - loss: 0.5412 - accuracy: 0.734 - 39s 77ms/step - loss: 0.5411 - accuracy: 0.734 - 39s 77ms/step - loss: 0.5409 - accuracy: 0.734 - 39s 77ms/step - loss: 0.5406 - accuracy: 0.735 - 39s 77ms/step - loss: 0.5403 - accuracy: 0.735 - 39s 77ms/step - loss: 0.5399 - accuracy: 0.735 - 39s 77ms/step - loss: 0.5396 - accuracy: 0.735 - 39s 77ms/step - loss: 0.5394 - accuracy: 0.735 - 39s 77ms/step - loss: 0.5390 - accuracy: 0.735 - 39s 77ms/step - loss: 0.5388 - accuracy: 0.735 - 39s 77ms/step - loss: 0.5385 - accuracy: 0.736 - 40s 77ms/step - loss: 0.5382 - accuracy: 0.736 - 40s 77ms/step - loss: 0.5381 - accuracy: 0.736 - 40s 77ms/step - loss: 0.5378 - accuracy: 0.736 - 40s 76ms/step - loss: 0.5376 - accuracy: 0.736 - 40s 76ms/step - loss: 0.5372 - accuracy: 0.737 - 40s 76ms/step - loss: 0.5368 - accuracy: 0.737 - 40s 76ms/step - loss: 0.5364 - accuracy: 0.737 - 40s 76ms/step - loss: 0.5365 - accuracy: 0.737 - 40s 76ms/step - loss: 0.5363 - accuracy: 0.737 - 40s 76ms/step - loss: 0.5359 - accuracy: 0.737 - 40s 76ms/step - loss: 0.5357 - accuracy: 0.737 - 40s 76ms/step - loss: 0.5356 - accuracy: 0.737 - 40s 76ms/step - loss: 0.5353 - accuracy: 0.738 - 40s 76ms/step - loss: 0.5352 - accuracy: 0.738 - 40s 76ms/step - loss: 0.5349 - accuracy: 0.738 - 40s 76ms/step - loss: 0.5346 - accuracy: 0.738 - 40s 76ms/step - loss: 0.5342 - accuracy: 0.738 - 40s 76ms/step - loss: 0.5339 - accuracy: 0.738 - 40s 76ms/step - loss: 0.5338 - accuracy: 0.738 - 40s 76ms/step - loss: 0.5334 - accuracy: 0.739 - 41s 76ms/step - loss: 0.5333 - accuracy: 0.739 - 41s 76ms/step - loss: 0.5330 - accuracy: 0.739 - 41s 75ms/step - loss: 0.5326 - accuracy: 0.739 - 41s 75ms/step - loss: 0.5325 - accuracy: 0.739 - 41s 75ms/step - loss: 0.5322 - accuracy: 0.740 - 41s 75ms/step - loss: 0.5320 - accuracy: 0.740 - 41s 75ms/step - loss: 0.5317 - accuracy: 0.740 - 41s 75ms/step - loss: 0.5315 - accuracy: 0.740 - 41s 75ms/step - loss: 0.5313 - accuracy: 0.740 - 41s 75ms/step - loss: 0.5309 - accuracy: 0.740 - 41s 75ms/step - loss: 0.5305 - accuracy: 0.741 - 41s 75ms/step - loss: 0.5303 - accuracy: 0.741 - 41s 75ms/step - loss: 0.5301 - accuracy: 0.741 - 41s 75ms/step - loss: 0.5299 - accuracy: 0.741 - 41s 75ms/step - loss: 0.5297 - accuracy: 0.741 - 41s 75ms/step - loss: 0.5293 - accuracy: 0.741 - 41s 75ms/step - loss: 0.5289 - accuracy: 0.742 - 41s 75ms/step - loss: 0.5288 - accuracy: 0.742 - 41s 75ms/step - loss: 0.5285 - accuracy: 0.742 - 41s 75ms/step - loss: 0.5281 - accuracy: 0.742 - 41s 75ms/step - loss: 0.5278 - accuracy: 0.742 - 41s 74ms/step - loss: 0.5274 - accuracy: 0.743 - 42s 74ms/step - loss: 0.5271 - accuracy: 0.743 - 42s 74ms/step - loss: 0.5266 - accuracy: 0.743 - 42s 74ms/step - loss: 0.5265 - accuracy: 0.743 - 42s 74ms/step - loss: 0.5262 - accuracy: 0.743 - 42s 74ms/step - loss: 0.5262 - accuracy: 0.743 - 42s 74ms/step - loss: 0.5260 - accuracy: 0.743 - 42s 74ms/step - loss: 0.5257 - accuracy: 0.744 - 42s 74ms/step - loss: 0.5256 - accuracy: 0.744 - 42s 74ms/step - loss: 0.5252 - accuracy: 0.744 - 42s 74ms/step - loss: 0.5250 - accuracy: 0.744 - 42s 74ms/step - loss: 0.5248 - accuracy: 0.744 - 42s 74ms/step - loss: 0.5247 - accuracy: 0.744 - 42s 74ms/step - loss: 0.5244 - accuracy: 0.744 - 42s 74ms/step - loss: 0.5241 - accuracy: 0.745 - 42s 74ms/step - loss: 0.5239 - accuracy: 0.745 - 42s 74ms/step - loss: 0.5237 - accuracy: 0.745 - 42s 74ms/step - loss: 0.5237 - accuracy: 0.745 - 42s 74ms/step - loss: 0.5235 - accuracy: 0.745 - 42s 74ms/step - loss: 0.5234 - accuracy: 0.745 - 43s 74ms/step - loss: 0.5231 - accuracy: 0.745 - 43s 74ms/step - loss: 0.5229 - accuracy: 0.745 - 43s 74ms/step - loss: 0.5227 - accuracy: 0.746 - 43s 74ms/step - loss: 0.5224 - accuracy: 0.746 - 43s 74ms/step - loss: 0.5223 - accuracy: 0.746 - 43s 74ms/step - loss: 0.5220 - accuracy: 0.746 - 43s 74ms/step - loss: 0.5216 - accuracy: 0.746 - 43s 74ms/step - loss: 0.5212 - accuracy: 0.746 - 43s 73ms/step - loss: 0.5210 - accuracy: 0.746 - 43s 73ms/step - loss: 0.5206 - accuracy: 0.747 - 43s 73ms/step - loss: 0.5203 - accuracy: 0.747 - 43s 73ms/step - loss: 0.5201 - accuracy: 0.747 - 43s 73ms/step - loss: 0.5198 - accuracy: 0.747 - 43s 73ms/step - loss: 0.5196 - accuracy: 0.747 - 43s 73ms/step - loss: 0.5196 - accuracy: 0.748 - 43s 73ms/step - loss: 0.5192 - accuracy: 0.748 - 43s 73ms/step - loss: 0.5189 - accuracy: 0.748 - 43s 73ms/step - loss: 0.5188 - accuracy: 0.748 - 44s 73ms/step - loss: 0.5187 - accuracy: 0.748 - 44s 73ms/step - loss: 0.5186 - accuracy: 0.748 - 44s 73ms/step - loss: 0.5187 - accuracy: 0.748 - 44s 73ms/step - loss: 0.5183 - accuracy: 0.748 - 44s 73ms/step - loss: 0.5182 - accuracy: 0.748 - 44s 73ms/step - loss: 0.5178 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5179 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5178 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5176 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5176 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5176 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5173 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5169 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5167 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5165 - accuracy: 0.749 - 44s 73ms/step - loss: 0.5164 - accuracy: 0.750 - 44s 73ms/step - loss: 0.5160 - accuracy: 0.750 - 44s 73ms/step - loss: 0.5158 - accuracy: 0.750 - 44s 73ms/step - loss: 0.5158 - accuracy: 0.750 - 45s 73ms/step - loss: 0.5154 - accuracy: 0.750 - 45s 72ms/step - loss: 0.5153 - accuracy: 0.750 - 45s 72ms/step - loss: 0.5154 - accuracy: 0.750 - 45s 72ms/step - loss: 0.5153 - accuracy: 0.750 - 45s 72ms/step - loss: 0.5150 - accuracy: 0.750 - 45s 72ms/step - loss: 0.5148 - accuracy: 0.750 - 45s 72ms/step - loss: 0.5148 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5145 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5146 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5145 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5142 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5140 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5139 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5137 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5135 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5135 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5132 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5130 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5129 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5130 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5130 - accuracy: 0.751 - 45s 72ms/step - loss: 0.5128 - accuracy: 0.751 - 46s 72ms/step - loss: 0.5127 - accuracy: 0.751 - 46s 72ms/step - loss: 0.5126 - accuracy: 0.751 - 46s 71ms/step - loss: 0.5123 - accuracy: 0.752 - 46s 71ms/step - loss: 0.5119 - accuracy: 0.752 - 46s 71ms/step - loss: 0.5117 - accuracy: 0.752 - 46s 71ms/step - loss: 0.5116 - accuracy: 0.752 - 46s 71ms/step - loss: 0.5113 - accuracy: 0.752 - 46s 71ms/step - loss: 0.5111 - accuracy: 0.752 - 46s 71ms/step - loss: 0.5111 - accuracy: 0.752 - 46s 71ms/step - loss: 0.5108 - accuracy: 0.752 - 46s 71ms/step - loss: 0.5105 - accuracy: 0.753 - 46s 71ms/step - loss: 0.5101 - accuracy: 0.753 - 46s 71ms/step - loss: 0.5098 - accuracy: 0.753 - 46s 71ms/step - loss: 0.5095 - accuracy: 0.753 - 46s 71ms/step - loss: 0.5092 - accuracy: 0.753 - 46s 71ms/step - loss: 0.5090 - accuracy: 0.753 - 46s 71ms/step - loss: 0.5088 - accuracy: 0.754 - 46s 71ms/step - loss: 0.5086 - accuracy: 0.754 - 46s 71ms/step - loss: 0.5083 - accuracy: 0.754 - 46s 71ms/step - loss: 0.5079 - accuracy: 0.754 - 46s 71ms/step - loss: 0.5076 - accuracy: 0.754 - 47s 71ms/step - loss: 0.5075 - accuracy: 0.754 - 47s 71ms/step - loss: 0.5071 - accuracy: 0.755 - 47s 71ms/step - loss: 0.5069 - accuracy: 0.755 - 47s 71ms/step - loss: 0.5071 - accuracy: 0.755 - 47s 71ms/step - loss: 0.5069 - accuracy: 0.755 - 47s 71ms/step - loss: 0.5067 - accuracy: 0.755 - 47s 70ms/step - loss: 0.5066 - accuracy: 0.755 - 47s 70ms/step - loss: 0.5063 - accuracy: 0.755 - 47s 70ms/step - loss: 0.5062 - accuracy: 0.755 - 47s 70ms/step - loss: 0.5059 - accuracy: 0.755 - 47s 70ms/step - loss: 0.5060 - accuracy: 0.755 - 47s 70ms/step - loss: 0.5058 - accuracy: 0.755 - 47s 70ms/step - loss: 0.5055 - accuracy: 0.7560"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697/697 [==============================]0.5054 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5054 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5053 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5051 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5049 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5048 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5046 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5045 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5042 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5043 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5041 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5040 - accuracy: 0.756 - 47s 70ms/step - loss: 0.5039 - accuracy: 0.756 - 48s 70ms/step - loss: 0.5038 - accuracy: 0.756 - 48s 70ms/step - loss: 0.5037 - accuracy: 0.756 - 48s 70ms/step - loss: 0.5036 - accuracy: 0.756 - 48s 69ms/step - loss: 0.5035 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5034 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5033 - accuracy: 0.756 - 48s 69ms/step - loss: 0.5031 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5029 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5028 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5025 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5022 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5021 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5018 - accuracy: 0.757 - 48s 69ms/step - loss: 0.5016 - accuracy: 0.758 - 48s 69ms/step - loss: 0.5014 - accuracy: 0.758 - 53s 76ms/step - loss: 0.5014 - accuracy: 0.7581 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "697/697 [==============================] ETA: 15:45 - loss: 0.3799 - accuracy: 0.859 - ETA: 5:08 - loss: 0.2969 - accuracy: 0.901 - ETA: 3:01 - loss: 0.2772 - accuracy: 0.89 - ETA: 2:06 - loss: 0.2876 - accuracy: 0.87 - ETA: 1:36 - loss: 0.2791 - accuracy: 0.88 - ETA: 1:17 - loss: 0.2691 - accuracy: 0.88 - ETA: 1:03 - loss: 0.2703 - accuracy: 0.88 - ETA: 53s - loss: 0.2750 - accuracy: 0.8833 - ETA: 46s - loss: 0.2812 - accuracy: 0.882 - ETA: 40s - loss: 0.2762 - accuracy: 0.884 - ETA: 35s - loss: 0.2779 - accuracy: 0.883 - ETA: 31s - loss: 0.2704 - accuracy: 0.885 - ETA: 27s - loss: 0.2714 - accuracy: 0.885 - ETA: 25s - loss: 0.2703 - accuracy: 0.884 - ETA: 22s - loss: 0.2768 - accuracy: 0.883 - ETA: 20s - loss: 0.2783 - accuracy: 0.884 - ETA: 18s - loss: 0.2792 - accuracy: 0.884 - ETA: 16s - loss: 0.2783 - accuracy: 0.885 - ETA: 15s - loss: 0.2791 - accuracy: 0.883 - ETA: 13s - loss: 0.2842 - accuracy: 0.880 - ETA: 12s - loss: 0.2838 - accuracy: 0.880 - ETA: 11s - loss: 0.2822 - accuracy: 0.879 - ETA: 10s - loss: 0.2791 - accuracy: 0.881 - ETA: 9s - loss: 0.2786 - accuracy: 0.881 - ETA: 8s - loss: 0.2775 - accuracy: 0.88 - ETA: 7s - loss: 0.2728 - accuracy: 0.88 - ETA: 6s - loss: 0.2757 - accuracy: 0.88 - ETA: 6s - loss: 0.2785 - accuracy: 0.88 - ETA: 5s - loss: 0.2790 - accuracy: 0.88 - ETA: 4s - loss: 0.2834 - accuracy: 0.88 - ETA: 4s - loss: 0.2862 - accuracy: 0.87 - ETA: 3s - loss: 0.2867 - accuracy: 0.87 - ETA: 3s - loss: 0.2834 - accuracy: 0.88 - ETA: 2s - loss: 0.2856 - accuracy: 0.87 - ETA: 2s - loss: 0.2834 - accuracy: 0.87 - ETA: 1s - loss: 0.2838 - accuracy: 0.87 - ETA: 1s - loss: 0.2833 - accuracy: 0.87 - ETA: 0s - loss: 0.2825 - accuracy: 0.87 - ETA: 0s - loss: 0.2834 - accuracy: 0.87 - 38s 55ms/step - loss: 0.2895 - accuracy: 0.8724 - val_loss: 0.3508 - val_accuracy: 0.8384\n",
      "Epoch 3/5\n",
      "697/697 [==============================] ETA: 15:54 - loss: 0.2557 - accuracy: 0.890 - ETA: 5:11 - loss: 0.2230 - accuracy: 0.901 - ETA: 3:03 - loss: 0.2234 - accuracy: 0.90 - ETA: 2:08 - loss: 0.2074 - accuracy: 0.91 - ETA: 1:37 - loss: 0.2110 - accuracy: 0.91 - ETA: 1:17 - loss: 0.2043 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2023 - accuracy: 0.91 - ETA: 54s - loss: 0.2030 - accuracy: 0.9156 - ETA: 46s - loss: 0.1975 - accuracy: 0.919 - ETA: 40s - loss: 0.1952 - accuracy: 0.921 - ETA: 35s - loss: 0.1890 - accuracy: 0.922 - ETA: 31s - loss: 0.1871 - accuracy: 0.924 - ETA: 28s - loss: 0.1822 - accuracy: 0.926 - ETA: 25s - loss: 0.1814 - accuracy: 0.927 - ETA: 22s - loss: 0.1806 - accuracy: 0.927 - ETA: 20s - loss: 0.1782 - accuracy: 0.928 - ETA: 18s - loss: 0.1805 - accuracy: 0.927 - ETA: 16s - loss: 0.1834 - accuracy: 0.926 - ETA: 15s - loss: 0.1826 - accuracy: 0.926 - ETA: 13s - loss: 0.1811 - accuracy: 0.927 - ETA: 12s - loss: 0.1801 - accuracy: 0.927 - ETA: 11s - loss: 0.1772 - accuracy: 0.929 - ETA: 10s - loss: 0.1781 - accuracy: 0.927 - ETA: 9s - loss: 0.1787 - accuracy: 0.927 - ETA: 8s - loss: 0.1788 - accuracy: 0.92 - ETA: 7s - loss: 0.1819 - accuracy: 0.92 - ETA: 6s - loss: 0.1796 - accuracy: 0.92 - ETA: 6s - loss: 0.1816 - accuracy: 0.92 - ETA: 5s - loss: 0.1816 - accuracy: 0.92 - ETA: 4s - loss: 0.1809 - accuracy: 0.92 - ETA: 4s - loss: 0.1822 - accuracy: 0.92 - ETA: 3s - loss: 0.1823 - accuracy: 0.92 - ETA: 3s - loss: 0.1829 - accuracy: 0.92 - ETA: 2s - loss: 0.1837 - accuracy: 0.92 - ETA: 2s - loss: 0.1858 - accuracy: 0.92 - ETA: 1s - loss: 0.1850 - accuracy: 0.92 - ETA: 1s - loss: 0.1842 - accuracy: 0.92 - ETA: 0s - loss: 0.1849 - accuracy: 0.92 - ETA: 0s - loss: 0.1852 - accuracy: 0.92 - 38s 55ms/step - loss: 0.2144 - accuracy: 0.9050 - val_loss: 0.3810 - val_accuracy: 0.8328\n",
      "Epoch 4/5\n",
      "697/697 [==============================] ETA: 16:11 - loss: 0.1571 - accuracy: 0.937 - ETA: 5:17 - loss: 0.1457 - accuracy: 0.937 - ETA: 3:06 - loss: 0.1318 - accuracy: 0.94 - ETA: 2:10 - loss: 0.1537 - accuracy: 0.93 - ETA: 1:38 - loss: 0.1598 - accuracy: 0.93 - ETA: 1:19 - loss: 0.1592 - accuracy: 0.93 - ETA: 1:05 - loss: 0.1533 - accuracy: 0.93 - ETA: 55s - loss: 0.1533 - accuracy: 0.9354 - ETA: 47s - loss: 0.1472 - accuracy: 0.938 - ETA: 41s - loss: 0.1433 - accuracy: 0.940 - ETA: 36s - loss: 0.1414 - accuracy: 0.940 - ETA: 32s - loss: 0.1426 - accuracy: 0.938 - ETA: 28s - loss: 0.1409 - accuracy: 0.940 - ETA: 25s - loss: 0.1401 - accuracy: 0.939 - ETA: 23s - loss: 0.1405 - accuracy: 0.939 - ETA: 20s - loss: 0.1403 - accuracy: 0.940 - ETA: 18s - loss: 0.1401 - accuracy: 0.940 - ETA: 17s - loss: 0.1477 - accuracy: 0.938 - ETA: 15s - loss: 0.1469 - accuracy: 0.939 - ETA: 14s - loss: 0.1460 - accuracy: 0.940 - ETA: 12s - loss: 0.1451 - accuracy: 0.940 - ETA: 11s - loss: 0.1453 - accuracy: 0.940 - ETA: 10s - loss: 0.1454 - accuracy: 0.940 - ETA: 9s - loss: 0.1466 - accuracy: 0.940 - ETA: 8s - loss: 0.1460 - accuracy: 0.94 - ETA: 7s - loss: 0.1445 - accuracy: 0.94 - ETA: 6s - loss: 0.1444 - accuracy: 0.94 - ETA: 6s - loss: 0.1435 - accuracy: 0.94 - ETA: 5s - loss: 0.1410 - accuracy: 0.94 - ETA: 4s - loss: 0.1410 - accuracy: 0.94 - ETA: 4s - loss: 0.1386 - accuracy: 0.94 - ETA: 3s - loss: 0.1382 - accuracy: 0.94 - ETA: 3s - loss: 0.1398 - accuracy: 0.94 - ETA: 2s - loss: 0.1389 - accuracy: 0.94 - ETA: 2s - loss: 0.1388 - accuracy: 0.94 - ETA: 1s - loss: 0.1389 - accuracy: 0.94 - ETA: 1s - loss: 0.1378 - accuracy: 0.94 - ETA: 0s - loss: 0.1371 - accuracy: 0.94 - ETA: 0s - loss: 0.1382 - accuracy: 0.94 - 38s 55ms/step - loss: 0.1708 - accuracy: 0.9238 - val_loss: 0.4396 - val_accuracy: 0.8354\n",
      "Epoch 5/5\n",
      "697/697 [==============================] ETA: 15:51 - loss: 0.1120 - accuracy: 0.953 - ETA: 5:10 - loss: 0.1138 - accuracy: 0.958 - ETA: 3:02 - loss: 0.1107 - accuracy: 0.95 - ETA: 2:07 - loss: 0.1085 - accuracy: 0.95 - ETA: 1:36 - loss: 0.1030 - accuracy: 0.96 - ETA: 1:17 - loss: 0.1134 - accuracy: 0.95 - ETA: 1:03 - loss: 0.1107 - accuracy: 0.95 - ETA: 54s - loss: 0.1126 - accuracy: 0.9542 - ETA: 46s - loss: 0.1145 - accuracy: 0.952 - ETA: 40s - loss: 0.1176 - accuracy: 0.951 - ETA: 35s - loss: 0.1195 - accuracy: 0.949 - ETA: 31s - loss: 0.1188 - accuracy: 0.949 - ETA: 28s - loss: 0.1182 - accuracy: 0.950 - ETA: 25s - loss: 0.1195 - accuracy: 0.949 - ETA: 22s - loss: 0.1197 - accuracy: 0.948 - ETA: 20s - loss: 0.1257 - accuracy: 0.946 - ETA: 18s - loss: 0.1241 - accuracy: 0.947 - ETA: 16s - loss: 0.1251 - accuracy: 0.947 - ETA: 15s - loss: 0.1221 - accuracy: 0.948 - ETA: 13s - loss: 0.1222 - accuracy: 0.948 - ETA: 12s - loss: 0.1188 - accuracy: 0.950 - ETA: 11s - loss: 0.1217 - accuracy: 0.949 - ETA: 10s - loss: 0.1207 - accuracy: 0.950 - ETA: 9s - loss: 0.1240 - accuracy: 0.948 - ETA: 8s - loss: 0.1249 - accuracy: 0.94 - ETA: 7s - loss: 0.1251 - accuracy: 0.94 - ETA: 6s - loss: 0.1242 - accuracy: 0.94 - ETA: 6s - loss: 0.1236 - accuracy: 0.94 - ETA: 5s - loss: 0.1248 - accuracy: 0.94 - ETA: 4s - loss: 0.1246 - accuracy: 0.94 - ETA: 4s - loss: 0.1263 - accuracy: 0.94 - ETA: 3s - loss: 0.1257 - accuracy: 0.94 - ETA: 3s - loss: 0.1277 - accuracy: 0.94 - ETA: 2s - loss: 0.1264 - accuracy: 0.94 - ETA: 2s - loss: 0.1254 - accuracy: 0.94 - ETA: 1s - loss: 0.1260 - accuracy: 0.94 - ETA: 1s - loss: 0.1258 - accuracy: 0.94 - ETA: 0s - loss: 0.1263 - accuracy: 0.94 - ETA: 0s - loss: 0.1259 - accuracy: 0.94 - 38s 54ms/step - loss: 0.1384 - accuracy: 0.9380 - val_loss: 0.4630 - val_accuracy: 0.8368\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_data,epochs=5,validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================].1593 - accuracy: 0.92 - 2s 1s/step - loss: 0.2477 - accuracy: 0.89 - 2s 679ms/step - loss: 0.2943 - accuracy: 0.906 - 2s 517ms/step - loss: 0.2995 - accuracy: 0.898 - 2s 420ms/step - loss: 0.3645 - accuracy: 0.878 - 2s 355ms/step - loss: 0.3511 - accuracy: 0.875 - 2s 309ms/step - loss: 0.3258 - accuracy: 0.877 - 2s 274ms/step - loss: 0.3293 - accuracy: 0.871 - 2s 247ms/step - loss: 0.3370 - accuracy: 0.869 - 2s 226ms/step - loss: 0.3466 - accuracy: 0.867 - 2s 208ms/step - loss: 0.3380 - accuracy: 0.866 - 2s 193ms/step - loss: 0.3599 - accuracy: 0.862 - 2s 181ms/step - loss: 0.3613 - accuracy: 0.863 - 2s 170ms/step - loss: 0.3624 - accuracy: 0.859 - 2s 160ms/step - loss: 0.3647 - accuracy: 0.857 - 2s 152ms/step - loss: 0.3675 - accuracy: 0.855 - 2s 145ms/step - loss: 0.3778 - accuracy: 0.850 - 3s 139ms/step - loss: 0.3774 - accuracy: 0.851 - 3s 134ms/step - loss: 0.3845 - accuracy: 0.851 - 3s 129ms/step - loss: 0.3926 - accuracy: 0.850 - 3s 124ms/step - loss: 0.4019 - accuracy: 0.850 - 3s 120ms/step - loss: 0.4026 - accuracy: 0.850 - 3s 117ms/step - loss: 0.4051 - accuracy: 0.850 - 3s 113ms/step - loss: 0.3978 - accuracy: 0.852 - 3s 110ms/step - loss: 0.3978 - accuracy: 0.852 - 3s 107ms/step - loss: 0.4043 - accuracy: 0.851 - 3s 104ms/step - loss: 0.3986 - accuracy: 0.851 - 3s 101ms/step - loss: 0.3993 - accuracy: 0.852 - 3s 99ms/step - loss: 0.3989 - accuracy: 0.852 - 3s 97ms/step - loss: 0.4052 - accuracy: 0.85 - 3s 95ms/step - loss: 0.4104 - accuracy: 0.85 - 3s 93ms/step - loss: 0.4083 - accuracy: 0.84 - 3s 91ms/step - loss: 0.4068 - accuracy: 0.85 - 3s 89ms/step - loss: 0.4038 - accuracy: 0.85 - 3s 87ms/step - loss: 0.4107 - accuracy: 0.85 - 3s 86ms/step - loss: 0.4138 - accuracy: 0.85 - 3s 84ms/step - loss: 0.4201 - accuracy: 0.85 - 3s 83ms/step - loss: 0.4273 - accuracy: 0.84 - 3s 82ms/step - loss: 0.4238 - accuracy: 0.85 - 3s 81ms/step - loss: 0.4256 - accuracy: 0.84 - 3s 79ms/step - loss: 0.4288 - accuracy: 0.84 - 3s 78ms/step - loss: 0.4306 - accuracy: 0.84 - 3s 77ms/step - loss: 0.4344 - accuracy: 0.84 - 3s 76ms/step - loss: 0.4364 - accuracy: 0.84 - 3s 75ms/step - loss: 0.4320 - accuracy: 0.84 - 3s 74ms/step - loss: 0.4330 - accuracy: 0.84 - 3s 73ms/step - loss: 0.4338 - accuracy: 0.84 - 3s 72ms/step - loss: 0.4338 - accuracy: 0.84 - 4s 72ms/step - loss: 0.4366 - accuracy: 0.84 - 4s 71ms/step - loss: 0.4384 - accuracy: 0.84 - 4s 70ms/step - loss: 0.4413 - accuracy: 0.84 - 4s 69ms/step - loss: 0.4431 - accuracy: 0.84 - 4s 68ms/step - loss: 0.4417 - accuracy: 0.84 - 4s 68ms/step - loss: 0.4415 - accuracy: 0.84 - 4s 67ms/step - loss: 0.4447 - accuracy: 0.84 - 4s 66ms/step - loss: 0.4418 - accuracy: 0.84 - 4s 66ms/step - loss: 0.4478 - accuracy: 0.83 - 4s 65ms/step - loss: 0.4476 - accuracy: 0.84 - 4s 64ms/step - loss: 0.4458 - accuracy: 0.84 - 4s 64ms/step - loss: 0.4465 - accuracy: 0.84 - 4s 63ms/step - loss: 0.4451 - accuracy: 0.84 - 4s 63ms/step - loss: 0.4451 - accuracy: 0.84 - 4s 62ms/step - loss: 0.4493 - accuracy: 0.84 - 4s 61ms/step - loss: 0.4579 - accuracy: 0.83 - 4s 61ms/step - loss: 0.4576 - accuracy: 0.83 - 4s 60ms/step - loss: 0.4619 - accuracy: 0.83 - 4s 60ms/step - loss: 0.4644 - accuracy: 0.83 - 4s 60ms/step - loss: 0.4634 - accuracy: 0.83 - 4s 59ms/step - loss: 0.4614 - accuracy: 0.83 - 4s 59ms/step - loss: 0.4583 - accuracy: 0.83 - 4s 58ms/step - loss: 0.4583 - accuracy: 0.83 - 4s 58ms/step - loss: 0.4590 - accuracy: 0.83 - 4s 58ms/step - loss: 0.4632 - accuracy: 0.83 - 4s 57ms/step - loss: 0.4661 - accuracy: 0.83 - 4s 57ms/step - loss: 0.4682 - accuracy: 0.83 - 4s 57ms/step - loss: 0.4665 - accuracy: 0.83 - 4s 56ms/step - loss: 0.4657 - accuracy: 0.83 - 4s 56ms/step - loss: 0.4639 - accuracy: 0.83 - 4s 56ms/step - loss: 0.4630 - accuracy: 0.83 - 4s 56ms/step - loss: 0.4630 - accuracy: 0.8368\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval loss: 0.463, Eval accuracy: 0.837\n"
     ]
    }
   ],
   "source": [
    "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "model.add(tf.keras.layers.Embedding(vocab_size,embedding_dim)) # \n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "#model.add(AttentionLayer(128,50,3,0.01))\n",
    "model.add(AttentionWithContext())\n",
    "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         1717900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         84480     \n",
      "_________________________________________________________________\n",
      "attention_with_context (Atte (None, 128)               16640     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,831,631\n",
      "Trainable params: 1,831,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_2/embeddings:0' shape=(17179, 100) dtype=float32, numpy=\n",
       " array([[ 0.0349843 ,  0.04488614,  0.03097877, ..., -0.0277553 ,\n",
       "         -0.0226357 ,  0.04396429],\n",
       "        [-0.01445959,  0.04020934,  0.04250174, ...,  0.00063106,\n",
       "         -0.02241715, -0.01841908],\n",
       "        [-0.00345569, -0.03979648, -0.0403442 , ..., -0.02423195,\n",
       "         -0.00714089, -0.02077893],\n",
       "        ...,\n",
       "        [ 0.03167503, -0.02202144, -0.00708278, ...,  0.00565876,\n",
       "          0.02888315, -0.02882534],\n",
       "        [-0.03373444, -0.04459515,  0.01304585, ..., -0.00698472,\n",
       "         -0.03796037,  0.01844081],\n",
       "        [ 0.00186817,  0.0140711 ,  0.03505471, ...,  0.04577067,\n",
       "         -0.03590371,  0.02632571]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/forward_lstm_1/kernel:0' shape=(100, 256) dtype=float32, numpy=\n",
       " array([[-0.07243173,  0.01996113,  0.11412027, ...,  0.02014096,\n",
       "          0.0772936 , -0.10448751],\n",
       "        [-0.0416146 ,  0.02809347,  0.09636031, ..., -0.019767  ,\n",
       "         -0.02643193,  0.01585555],\n",
       "        [ 0.05670179,  0.04420093,  0.10442093, ..., -0.07098605,\n",
       "          0.10073549,  0.12058735],\n",
       "        ...,\n",
       "        [ 0.09720948,  0.11819419,  0.05544695, ..., -0.0247541 ,\n",
       "          0.00490363,  0.11003026],\n",
       "        [ 0.10998665, -0.1169004 ,  0.12105024, ..., -0.09124033,\n",
       "          0.05285759, -0.03645686],\n",
       "        [ 0.08884865,  0.07599296, -0.09739872, ..., -0.0580063 ,\n",
       "         -0.09555985,  0.10249963]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/forward_lstm_1/recurrent_kernel:0' shape=(64, 256) dtype=float32, numpy=\n",
       " array([[-0.03656852, -0.02290622, -0.0123651 , ..., -0.06438082,\n",
       "          0.04834802,  0.04553421],\n",
       "        [ 0.08982116,  0.06002069,  0.04013805, ..., -0.06409416,\n",
       "         -0.06188561, -0.05626584],\n",
       "        [-0.05602276,  0.09485029, -0.00947642, ..., -0.10576417,\n",
       "          0.01217999,  0.02077464],\n",
       "        ...,\n",
       "        [-0.08645209, -0.0254708 ,  0.049662  , ..., -0.07658104,\n",
       "          0.00200951, -0.0309885 ],\n",
       "        [ 0.04384002,  0.02084894, -0.06166146, ...,  0.0665881 ,\n",
       "          0.10175305, -0.06151599],\n",
       "        [-0.00212283, -0.01041069,  0.05779737, ...,  0.11423112,\n",
       "          0.02854621, -0.00102873]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/forward_lstm_1/bias:0' shape=(256,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/backward_lstm_1/kernel:0' shape=(100, 256) dtype=float32, numpy=\n",
       " array([[-0.06866003,  0.11014853, -0.00700376, ...,  0.00906721,\n",
       "          0.1131857 ,  0.05378544],\n",
       "        [ 0.12400746, -0.06852257,  0.04437061, ...,  0.00108267,\n",
       "         -0.10268807,  0.08094038],\n",
       "        [ 0.05641787, -0.10001108, -0.10980868, ...,  0.08705993,\n",
       "         -0.10885791,  0.04717194],\n",
       "        ...,\n",
       "        [-0.06324798,  0.1242947 ,  0.12895799, ..., -0.04076926,\n",
       "          0.05786812,  0.09382412],\n",
       "        [-0.05571921, -0.09209238,  0.11340794, ..., -0.06033155,\n",
       "          0.12327892, -0.0018905 ],\n",
       "        [ 0.10917012, -0.09944605,  0.10045961, ...,  0.03616206,\n",
       "          0.01773567,  0.10101148]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/backward_lstm_1/recurrent_kernel:0' shape=(64, 256) dtype=float32, numpy=\n",
       " array([[ 0.07425165, -0.02498108, -0.16467756, ...,  0.00112925,\n",
       "         -0.06047159, -0.02604017],\n",
       "        [ 0.00037784,  0.01402831, -0.05375364, ...,  0.05528758,\n",
       "         -0.02203741, -0.05839503],\n",
       "        [ 0.05270931, -0.05925746,  0.00286865, ..., -0.02337704,\n",
       "          0.12317599,  0.01321528],\n",
       "        ...,\n",
       "        [-0.08103725,  0.07696375, -0.03107104, ...,  0.0187022 ,\n",
       "         -0.03168225,  0.04705966],\n",
       "        [-0.03072929, -0.1253228 ,  0.14130987, ..., -0.02637356,\n",
       "          0.10161569, -0.04644718],\n",
       "        [-0.05224136, -0.04916754,  0.03173176, ..., -0.0462736 ,\n",
       "          0.06907564, -0.06841026]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/backward_lstm_1/bias:0' shape=(256,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32)>,\n",
       " <tf.Variable 'attention_with_context/attention_with_context_W:0' shape=(128, 128) dtype=float32, numpy=\n",
       " array([[-0.10628086,  0.12666486, -0.09504729, ...,  0.08344696,\n",
       "         -0.07324351, -0.08627661],\n",
       "        [ 0.05801491,  0.13040985,  0.13010438, ..., -0.06658834,\n",
       "         -0.04214866,  0.10094135],\n",
       "        [ 0.10625313, -0.02995287,  0.09842007, ...,  0.00898091,\n",
       "          0.0982158 ,  0.12746386],\n",
       "        ...,\n",
       "        [-0.00196715,  0.01685099, -0.08635545, ...,  0.05011921,\n",
       "          0.00235565,  0.02183992],\n",
       "        [ 0.1284648 , -0.15224251, -0.12917702, ...,  0.09418635,\n",
       "          0.05420358, -0.01731881],\n",
       "        [-0.11119379,  0.07213715,  0.02750725, ...,  0.1025974 ,\n",
       "          0.0799713 , -0.11888545]], dtype=float32)>,\n",
       " <tf.Variable 'attention_with_context/attention_with_context_b:0' shape=(128,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'attention_with_context/attention_with_context_u:0' shape=(128,) dtype=float32, numpy=\n",
       " array([ 1.34265319e-01, -1.17147788e-01, -4.84751314e-02,  1.41533539e-01,\n",
       "        -2.92191729e-02, -8.66674557e-02, -2.21642256e-02,  1.19327798e-01,\n",
       "        -9.70261544e-02,  1.04325578e-01,  7.07777292e-02, -1.44034624e-04,\n",
       "         4.59715426e-02, -1.13712251e-03, -1.46257877e-02, -1.12405166e-01,\n",
       "         8.42368305e-02, -1.20993450e-01, -1.20037660e-01, -1.81269646e-02,\n",
       "         1.18929848e-01,  2.71358192e-02, -1.52856112e-02,  1.41395763e-01,\n",
       "         1.37761876e-01, -4.66861129e-02,  5.22214770e-02,  1.37384221e-01,\n",
       "        -1.31768584e-01, -9.98919308e-02,  2.59718150e-02,  1.60188973e-03,\n",
       "        -3.93873006e-02, -9.16768610e-03, -6.84397072e-02, -1.32133618e-01,\n",
       "        -2.72743702e-02, -3.40262949e-02,  1.07531175e-01,  2.02088654e-02,\n",
       "        -5.01716286e-02, -1.50849968e-02,  9.04071927e-02,  4.78299111e-02,\n",
       "        -1.31728679e-01, -6.87425137e-02,  1.24095872e-01, -7.51429051e-02,\n",
       "        -2.16849446e-02, -5.54174781e-02, -1.74429566e-02, -5.42790741e-02,\n",
       "        -1.11924961e-01, -1.08553655e-01,  3.07426602e-02, -3.24759483e-02,\n",
       "         7.83853978e-02, -1.81646794e-02, -1.17308244e-01,  1.44188270e-01,\n",
       "         1.48016214e-02, -9.02376920e-02,  3.11031342e-02, -1.84005052e-02,\n",
       "        -8.70323479e-02,  1.25125125e-01,  1.31862715e-01, -2.99146846e-02,\n",
       "        -4.89696413e-02, -1.38817683e-01, -1.21424407e-01,  1.90431923e-02,\n",
       "        -1.40202761e-01, -5.12287095e-02,  9.63385999e-02,  1.41066298e-01,\n",
       "         1.05237231e-01, -2.35732794e-02,  2.38778442e-02,  6.06536567e-02,\n",
       "        -9.15367901e-03,  4.27318215e-02, -7.92123154e-02,  1.16839513e-01,\n",
       "         3.51204574e-02,  1.68608427e-02, -5.31011000e-02, -1.51511043e-01,\n",
       "        -9.24021900e-02, -9.75647569e-03,  1.22005627e-01, -9.42093581e-02,\n",
       "        -5.45417294e-02,  3.65691483e-02,  9.84981209e-02,  1.22939214e-01,\n",
       "        -1.47355124e-01,  1.12163886e-01, -5.47238290e-02, -8.36080760e-02,\n",
       "         1.14096701e-03, -9.79748294e-02,  1.76588595e-02,  2.58020610e-02,\n",
       "        -4.02949154e-02,  1.51777193e-01,  7.28950351e-02,  3.72121036e-02,\n",
       "         9.08647925e-02,  6.25617504e-02, -3.25427428e-02,  7.66612440e-02,\n",
       "        -9.56042856e-02, -2.60193497e-02, -1.30571693e-01, -4.68617827e-02,\n",
       "        -1.11373261e-01,  5.21850884e-02,  6.72918111e-02,  1.56246126e-03,\n",
       "        -1.09857231e-01,  9.03480947e-02, -1.12262011e-01,  1.77514553e-03,\n",
       "        -1.03334561e-01, -7.29934797e-02,  7.53481835e-02, -1.42949402e-01],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
       " array([[-0.02164768,  0.05004267,  0.00085452, ..., -0.14104947,\n",
       "          0.03689978, -0.02984098],\n",
       "        [ 0.14269583,  0.10395475,  0.0819342 , ...,  0.1707692 ,\n",
       "         -0.16059463, -0.15639064],\n",
       "        [ 0.06835489, -0.05813079, -0.01344889, ...,  0.03550139,\n",
       "         -0.0794431 ,  0.14568816],\n",
       "        ...,\n",
       "        [ 0.16150768, -0.14248379,  0.0516199 , ...,  0.04574446,\n",
       "          0.01045828, -0.10835993],\n",
       "        [ 0.09878327,  0.04568529,  0.08059408, ...,  0.03254461,\n",
       "         -0.16623645, -0.0153973 ],\n",
       "        [ 0.09326555, -0.01245132, -0.05158597, ...,  0.08648671,\n",
       "          0.14912255, -0.02552705]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.11775555,  0.18147068,  0.01867916, ...,  0.1694441 ,\n",
       "         -0.15175447,  0.17993848],\n",
       "        [-0.10050008, -0.18733048, -0.16426484, ...,  0.20883419,\n",
       "         -0.01625869, -0.00338885],\n",
       "        [-0.1483184 ,  0.00113417,  0.16126509, ...,  0.13719071,\n",
       "         -0.02118449,  0.13757707],\n",
       "        ...,\n",
       "        [ 0.16156672, -0.18321587, -0.18946855, ..., -0.14845864,\n",
       "         -0.08018914,  0.19795914],\n",
       "        [ 0.16420095,  0.00876203, -0.13740787, ...,  0.08371581,\n",
       "         -0.08162844,  0.06664158],\n",
       "        [ 0.2115648 ,  0.10655703,  0.20393042, ...,  0.16159774,\n",
       "          0.21510138,  0.12362762]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(64, 3) dtype=float32, numpy=\n",
       " array([[ 0.04756948,  0.13053733, -0.1671955 ],\n",
       "        [ 0.16948453, -0.11546732, -0.28392908],\n",
       "        [-0.2304806 ,  0.15383768, -0.08239986],\n",
       "        [ 0.07427925, -0.10146725, -0.0865064 ],\n",
       "        [-0.03095302,  0.09432986,  0.1985504 ],\n",
       "        [ 0.00364009,  0.07140842,  0.10660741],\n",
       "        [ 0.14313835,  0.2572056 ,  0.13948852],\n",
       "        [ 0.2871204 ,  0.0702135 ,  0.06041771],\n",
       "        [ 0.15650558, -0.2924354 , -0.09162229],\n",
       "        [-0.1573499 ,  0.0712676 ,  0.21383446],\n",
       "        [ 0.21650165,  0.12635723, -0.10979064],\n",
       "        [ 0.00358599, -0.18489593,  0.2865501 ],\n",
       "        [-0.02669472,  0.20277733, -0.27672842],\n",
       "        [ 0.10412294,  0.14936626,  0.29533428],\n",
       "        [ 0.28118545, -0.08720568, -0.09601787],\n",
       "        [-0.21229257,  0.05270079,  0.22149861],\n",
       "        [ 0.20850837,  0.21363169, -0.10280101],\n",
       "        [-0.18349902,  0.06894231,  0.12406513],\n",
       "        [-0.10990252,  0.20764887, -0.28328848],\n",
       "        [-0.19200706,  0.12192255,  0.06966513],\n",
       "        [ 0.01359639,  0.10417473,  0.1957244 ],\n",
       "        [ 0.05747527,  0.2044658 , -0.2721921 ],\n",
       "        [ 0.24331027, -0.2611777 ,  0.28404218],\n",
       "        [ 0.26985425, -0.28929505, -0.08201493],\n",
       "        [-0.26626226, -0.2077814 , -0.29028395],\n",
       "        [ 0.09869325,  0.1761944 ,  0.00094   ],\n",
       "        [ 0.17579114, -0.27077782, -0.23232171],\n",
       "        [ 0.18732923,  0.24870509,  0.12006032],\n",
       "        [ 0.08145386,  0.03275311, -0.05243993],\n",
       "        [-0.04304689,  0.07168755,  0.10845816],\n",
       "        [ 0.2002339 , -0.05373076,  0.16144982],\n",
       "        [ 0.26453698, -0.16375504, -0.28873298],\n",
       "        [-0.00194836,  0.27321512,  0.14244044],\n",
       "        [ 0.2706622 , -0.08123055,  0.08785051],\n",
       "        [ 0.26451266,  0.13790873,  0.14307243],\n",
       "        [ 0.10924298,  0.02486238,  0.1696975 ],\n",
       "        [ 0.08076742, -0.15611574, -0.16523144],\n",
       "        [-0.28814816, -0.10325071, -0.00040674],\n",
       "        [-0.16823931,  0.20695972, -0.22873572],\n",
       "        [-0.03473601,  0.01337558,  0.00967607],\n",
       "        [-0.04817563,  0.1796442 ,  0.20151591],\n",
       "        [-0.04282379,  0.19367245, -0.24794994],\n",
       "        [ 0.23662233,  0.10338807,  0.1853188 ],\n",
       "        [ 0.20845842,  0.04558766,  0.15089282],\n",
       "        [-0.00907654, -0.13635863, -0.19535461],\n",
       "        [ 0.1723868 , -0.20616162,  0.13214734],\n",
       "        [-0.04326281,  0.24138725,  0.00509685],\n",
       "        [ 0.0427438 ,  0.10009488, -0.2822752 ],\n",
       "        [ 0.29349262, -0.29649058,  0.11687359],\n",
       "        [-0.09545672,  0.1523951 , -0.24105549],\n",
       "        [-0.0318948 ,  0.28000206, -0.01264477],\n",
       "        [-0.18117487, -0.2881876 , -0.27671087],\n",
       "        [ 0.27769977, -0.25158346,  0.10641092],\n",
       "        [ 0.29806274,  0.27804297,  0.20808798],\n",
       "        [ 0.17062452,  0.09260997, -0.25395513],\n",
       "        [ 0.29153824, -0.02387172, -0.06178831],\n",
       "        [ 0.29464895, -0.16409452, -0.23519132],\n",
       "        [-0.18876532,  0.29166824, -0.19910692],\n",
       "        [ 0.2687322 ,  0.09262195,  0.1899178 ],\n",
       "        [ 0.1584281 ,  0.09319621,  0.14832702],\n",
       "        [-0.16339675, -0.12315658, -0.16476697],\n",
       "        [-0.09498782,  0.02363113,  0.01611474],\n",
       "        [-0.27827287,  0.08213979, -0.18224901],\n",
       "        [ 0.20652461,  0.19409275, -0.04435262]], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    168/Unknown - 15s 15s/step - loss: 1.0978 - accuracy: 0.42 - 15s 7s/step - loss: 1.0959 - accuracy: 0.3906 - 15s 5s/step - loss: 1.0931 - accuracy: 0.427 - 15s 4s/step - loss: 1.0912 - accuracy: 0.433 - 15s 3s/step - loss: 1.0910 - accuracy: 0.421 - 15s 3s/step - loss: 1.0881 - accuracy: 0.427 - 15s 2s/step - loss: 1.0858 - accuracy: 0.412 - 15s 2s/step - loss: 1.0830 - accuracy: 0.406 - 15s 2s/step - loss: 1.0795 - accuracy: 0.411 - 15s 2s/step - loss: 1.0766 - accuracy: 0.404 - 15s 1s/step - loss: 1.0738 - accuracy: 0.394 - 15s 1s/step - loss: 1.0762 - accuracy: 0.384 - 15s 1s/step - loss: 1.0724 - accuracy: 0.388 - 15s 1s/step - loss: 1.0664 - accuracy: 0.395 - 15s 1s/step - loss: 1.0639 - accuracy: 0.389 - 15s 964ms/step - loss: 1.0572 - accuracy: 0.39 - 15s 910ms/step - loss: 1.0525 - accuracy: 0.39 - 15s 861ms/step - loss: 1.0490 - accuracy: 0.38 - 16s 818ms/step - loss: 1.0433 - accuracy: 0.38 - 16s 778ms/step - loss: 1.0378 - accuracy: 0.38 - 16s 743ms/step - loss: 1.0320 - accuracy: 0.39 - 16s 711ms/step - loss: 1.0248 - accuracy: 0.39 - 16s 682ms/step - loss: 1.0166 - accuracy: 0.40 - 16s 655ms/step - loss: 1.0125 - accuracy: 0.40 - 16s 630ms/step - loss: 1.0094 - accuracy: 0.40 - 16s 607ms/step - loss: 1.0021 - accuracy: 0.41 - 16s 586ms/step - loss: 0.9996 - accuracy: 0.42 - 16s 566ms/step - loss: 0.9953 - accuracy: 0.42 - 16s 548ms/step - loss: 0.9900 - accuracy: 0.43 - 16s 531ms/step - loss: 0.9861 - accuracy: 0.44 - 16s 515ms/step - loss: 0.9795 - accuracy: 0.45 - 16s 500ms/step - loss: 0.9764 - accuracy: 0.45 - 16s 486ms/step - loss: 0.9698 - accuracy: 0.46 - 16s 473ms/step - loss: 0.9644 - accuracy: 0.47 - 16s 461ms/step - loss: 0.9575 - accuracy: 0.47 - 16s 449ms/step - loss: 0.9505 - accuracy: 0.48 - 16s 438ms/step - loss: 0.9461 - accuracy: 0.48 - 16s 427ms/step - loss: 0.9428 - accuracy: 0.48 - 16s 417ms/step - loss: 0.9376 - accuracy: 0.49 - 16s 407ms/step - loss: 0.9329 - accuracy: 0.49 - 16s 398ms/step - loss: 0.9255 - accuracy: 0.49 - 16s 390ms/step - loss: 0.9187 - accuracy: 0.49 - 16s 381ms/step - loss: 0.9122 - accuracy: 0.50 - 16s 373ms/step - loss: 0.9058 - accuracy: 0.50 - 16s 366ms/step - loss: 0.9000 - accuracy: 0.50 - 17s 359ms/step - loss: 0.8966 - accuracy: 0.50 - 17s 352ms/step - loss: 0.8924 - accuracy: 0.50 - 17s 345ms/step - loss: 0.8882 - accuracy: 0.51 - 17s 339ms/step - loss: 0.8838 - accuracy: 0.51 - 17s 333ms/step - loss: 0.8789 - accuracy: 0.51 - 17s 327ms/step - loss: 0.8732 - accuracy: 0.52 - 17s 322ms/step - loss: 0.8787 - accuracy: 0.52 - 17s 316ms/step - loss: 0.8747 - accuracy: 0.52 - 17s 311ms/step - loss: 0.8697 - accuracy: 0.52 - 17s 306ms/step - loss: 0.8678 - accuracy: 0.52 - 17s 301ms/step - loss: 0.8629 - accuracy: 0.52 - 17s 297ms/step - loss: 0.8599 - accuracy: 0.53 - 17s 292ms/step - loss: 0.8585 - accuracy: 0.53 - 17s 288ms/step - loss: 0.8520 - accuracy: 0.53 - 17s 283ms/step - loss: 0.8486 - accuracy: 0.53 - 17s 279ms/step - loss: 0.8452 - accuracy: 0.53 - 17s 276ms/step - loss: 0.8434 - accuracy: 0.53 - 17s 272ms/step - loss: 0.8399 - accuracy: 0.53 - 17s 268ms/step - loss: 0.8371 - accuracy: 0.54 - 17s 264ms/step - loss: 0.8334 - accuracy: 0.54 - 17s 261ms/step - loss: 0.8290 - accuracy: 0.54 - 17s 258ms/step - loss: 0.8250 - accuracy: 0.54 - 17s 254ms/step - loss: 0.8224 - accuracy: 0.54 - 17s 251ms/step - loss: 0.8189 - accuracy: 0.55 - 17s 248ms/step - loss: 0.8182 - accuracy: 0.55 - 17s 245ms/step - loss: 0.8165 - accuracy: 0.55 - 17s 242ms/step - loss: 0.8149 - accuracy: 0.55 - 17s 239ms/step - loss: 0.8109 - accuracy: 0.55 - 17s 236ms/step - loss: 0.8083 - accuracy: 0.55 - 18s 234ms/step - loss: 0.8065 - accuracy: 0.55 - 18s 231ms/step - loss: 0.8043 - accuracy: 0.56 - 18s 228ms/step - loss: 0.8027 - accuracy: 0.56 - 18s 226ms/step - loss: 0.8014 - accuracy: 0.56 - 18s 224ms/step - loss: 0.8001 - accuracy: 0.56 - 18s 221ms/step - loss: 0.7988 - accuracy: 0.56 - 18s 219ms/step - loss: 0.7962 - accuracy: 0.56 - 18s 217ms/step - loss: 0.7937 - accuracy: 0.56 - 18s 214ms/step - loss: 0.7929 - accuracy: 0.56 - 18s 212ms/step - loss: 0.7898 - accuracy: 0.56 - 18s 210ms/step - loss: 0.7881 - accuracy: 0.56 - 18s 208ms/step - loss: 0.7861 - accuracy: 0.56 - 18s 206ms/step - loss: 0.7841 - accuracy: 0.57 - 18s 204ms/step - loss: 0.7828 - accuracy: 0.57 - 18s 202ms/step - loss: 0.7800 - accuracy: 0.57 - 18s 200ms/step - loss: 0.7791 - accuracy: 0.57 - 18s 199ms/step - loss: 0.7778 - accuracy: 0.57 - 18s 197ms/step - loss: 0.7759 - accuracy: 0.57 - 18s 195ms/step - loss: 0.7740 - accuracy: 0.57 - 18s 193ms/step - loss: 0.7713 - accuracy: 0.57 - 18s 192ms/step - loss: 0.7706 - accuracy: 0.58 - 18s 190ms/step - loss: 0.7703 - accuracy: 0.58 - 18s 188ms/step - loss: 0.7678 - accuracy: 0.58 - 18s 187ms/step - loss: 0.7653 - accuracy: 0.58 - 18s 185ms/step - loss: 0.7657 - accuracy: 0.58 - 18s 184ms/step - loss: 0.7632 - accuracy: 0.58 - 18s 183ms/step - loss: 0.7622 - accuracy: 0.58 - 18s 181ms/step - loss: 0.7615 - accuracy: 0.58 - 19s 180ms/step - loss: 0.7598 - accuracy: 0.58 - 19s 178ms/step - loss: 0.7583 - accuracy: 0.58 - 19s 177ms/step - loss: 0.7562 - accuracy: 0.58 - 19s 176ms/step - loss: 0.7548 - accuracy: 0.58 - 19s 174ms/step - loss: 0.7527 - accuracy: 0.58 - 19s 173ms/step - loss: 0.7512 - accuracy: 0.59 - 19s 172ms/step - loss: 0.7514 - accuracy: 0.59 - 19s 170ms/step - loss: 0.7499 - accuracy: 0.59 - 19s 169ms/step - loss: 0.7502 - accuracy: 0.59 - 19s 168ms/step - loss: 0.7479 - accuracy: 0.59 - 19s 167ms/step - loss: 0.7466 - accuracy: 0.59 - 19s 166ms/step - loss: 0.7457 - accuracy: 0.59 - 19s 165ms/step - loss: 0.7449 - accuracy: 0.59 - 19s 163ms/step - loss: 0.7442 - accuracy: 0.59 - 19s 162ms/step - loss: 0.7427 - accuracy: 0.59 - 19s 161ms/step - loss: 0.7413 - accuracy: 0.59 - 19s 160ms/step - loss: 0.7400 - accuracy: 0.59 - 19s 159ms/step - loss: 0.7400 - accuracy: 0.59 - 19s 158ms/step - loss: 0.7387 - accuracy: 0.59 - 19s 157ms/step - loss: 0.7375 - accuracy: 0.60 - 19s 156ms/step - loss: 0.7371 - accuracy: 0.60 - 19s 155ms/step - loss: 0.7367 - accuracy: 0.60 - 19s 154ms/step - loss: 0.7350 - accuracy: 0.60 - 19s 153ms/step - loss: 0.7335 - accuracy: 0.60 - 19s 152ms/step - loss: 0.7333 - accuracy: 0.60 - 19s 151ms/step - loss: 0.7323 - accuracy: 0.60 - 19s 150ms/step - loss: 0.7310 - accuracy: 0.60 - 19s 149ms/step - loss: 0.7302 - accuracy: 0.60 - 19s 148ms/step - loss: 0.7290 - accuracy: 0.60 - 19s 148ms/step - loss: 0.7278 - accuracy: 0.61 - 20s 147ms/step - loss: 0.7272 - accuracy: 0.61 - 20s 146ms/step - loss: 0.7267 - accuracy: 0.61 - 20s 145ms/step - loss: 0.7257 - accuracy: 0.61 - 20s 144ms/step - loss: 0.7257 - accuracy: 0.61 - 20s 143ms/step - loss: 0.7254 - accuracy: 0.61 - 20s 143ms/step - loss: 0.7240 - accuracy: 0.61 - 20s 142ms/step - loss: 0.7229 - accuracy: 0.61 - 20s 141ms/step - loss: 0.7232 - accuracy: 0.61 - 20s 140ms/step - loss: 0.7228 - accuracy: 0.61 - 20s 139ms/step - loss: 0.7211 - accuracy: 0.61 - 20s 139ms/step - loss: 0.7209 - accuracy: 0.61 - 20s 138ms/step - loss: 0.7192 - accuracy: 0.61 - 20s 137ms/step - loss: 0.7187 - accuracy: 0.61 - 20s 137ms/step - loss: 0.7173 - accuracy: 0.61 - 20s 136ms/step - loss: 0.7177 - accuracy: 0.61 - 20s 135ms/step - loss: 0.7176 - accuracy: 0.61 - 20s 134ms/step - loss: 0.7168 - accuracy: 0.61 - 20s 134ms/step - loss: 0.7160 - accuracy: 0.61 - 20s 133ms/step - loss: 0.7153 - accuracy: 0.61 - 20s 132ms/step - loss: 0.7148 - accuracy: 0.61 - 20s 132ms/step - loss: 0.7134 - accuracy: 0.61 - 20s 131ms/step - loss: 0.7125 - accuracy: 0.61 - 20s 130ms/step - loss: 0.7116 - accuracy: 0.61 - 20s 130ms/step - loss: 0.7109 - accuracy: 0.61 - 20s 129ms/step - loss: 0.7108 - accuracy: 0.62 - 20s 129ms/step - loss: 0.7111 - accuracy: 0.61 - 20s 128ms/step - loss: 0.7101 - accuracy: 0.62 - 20s 127ms/step - loss: 0.7086 - accuracy: 0.62 - 20s 127ms/step - loss: 0.7084 - accuracy: 0.62 - 20s 126ms/step - loss: 0.7075 - accuracy: 0.62 - 20s 126ms/step - loss: 0.7069 - accuracy: 0.62 - 21s 125ms/step - loss: 0.7063 - accuracy: 0.62 - 21s 125ms/step - loss: 0.7058 - accuracy: 0.62 - 21s 124ms/step - loss: 0.7052 - accuracy: 0.62 - 21s 123ms/step - loss: 0.7047 - accuracy: 0.62 - 21s 123ms/step - loss: 0.7051 - accuracy: 0.6251\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    335/Unknown - 21s 122ms/step - loss: 0.7040 - accuracy: 0.62 - 21s 122ms/step - loss: 0.7047 - accuracy: 0.62 - 21s 121ms/step - loss: 0.7036 - accuracy: 0.62 - 21s 121ms/step - loss: 0.7032 - accuracy: 0.62 - 21s 120ms/step - loss: 0.7023 - accuracy: 0.62 - 21s 120ms/step - loss: 0.7016 - accuracy: 0.62 - 21s 119ms/step - loss: 0.7000 - accuracy: 0.62 - 21s 119ms/step - loss: 0.6995 - accuracy: 0.62 - 21s 118ms/step - loss: 0.6994 - accuracy: 0.62 - 21s 118ms/step - loss: 0.6993 - accuracy: 0.62 - 21s 117ms/step - loss: 0.6986 - accuracy: 0.62 - 21s 117ms/step - loss: 0.6978 - accuracy: 0.62 - 21s 116ms/step - loss: 0.6973 - accuracy: 0.63 - 21s 116ms/step - loss: 0.6968 - accuracy: 0.63 - 21s 116ms/step - loss: 0.6961 - accuracy: 0.63 - 21s 115ms/step - loss: 0.6957 - accuracy: 0.63 - 21s 115ms/step - loss: 0.6951 - accuracy: 0.63 - 21s 114ms/step - loss: 0.6944 - accuracy: 0.63 - 21s 114ms/step - loss: 0.6940 - accuracy: 0.63 - 21s 113ms/step - loss: 0.6934 - accuracy: 0.63 - 21s 113ms/step - loss: 0.6931 - accuracy: 0.63 - 21s 113ms/step - loss: 0.6929 - accuracy: 0.63 - 21s 112ms/step - loss: 0.6925 - accuracy: 0.63 - 21s 112ms/step - loss: 0.6923 - accuracy: 0.63 - 21s 111ms/step - loss: 0.6917 - accuracy: 0.63 - 22s 111ms/step - loss: 0.6918 - accuracy: 0.63 - 22s 111ms/step - loss: 0.6918 - accuracy: 0.63 - 22s 110ms/step - loss: 0.6909 - accuracy: 0.63 - 22s 110ms/step - loss: 0.6902 - accuracy: 0.63 - 22s 109ms/step - loss: 0.6897 - accuracy: 0.63 - 22s 109ms/step - loss: 0.6895 - accuracy: 0.63 - 22s 109ms/step - loss: 0.6890 - accuracy: 0.63 - 22s 108ms/step - loss: 0.6884 - accuracy: 0.63 - 22s 108ms/step - loss: 0.6876 - accuracy: 0.63 - 22s 108ms/step - loss: 0.6877 - accuracy: 0.63 - 22s 107ms/step - loss: 0.6871 - accuracy: 0.63 - 22s 107ms/step - loss: 0.6863 - accuracy: 0.63 - 22s 107ms/step - loss: 0.6856 - accuracy: 0.63 - 22s 106ms/step - loss: 0.6846 - accuracy: 0.64 - 22s 106ms/step - loss: 0.6838 - accuracy: 0.64 - 22s 106ms/step - loss: 0.6828 - accuracy: 0.64 - 22s 105ms/step - loss: 0.6828 - accuracy: 0.64 - 22s 105ms/step - loss: 0.6835 - accuracy: 0.64 - 22s 105ms/step - loss: 0.6825 - accuracy: 0.64 - 22s 104ms/step - loss: 0.6817 - accuracy: 0.64 - 22s 104ms/step - loss: 0.6814 - accuracy: 0.64 - 22s 104ms/step - loss: 0.6812 - accuracy: 0.64 - 22s 103ms/step - loss: 0.6808 - accuracy: 0.64 - 22s 103ms/step - loss: 0.6807 - accuracy: 0.64 - 22s 103ms/step - loss: 0.6802 - accuracy: 0.64 - 22s 102ms/step - loss: 0.6797 - accuracy: 0.64 - 22s 102ms/step - loss: 0.6798 - accuracy: 0.64 - 23s 102ms/step - loss: 0.6799 - accuracy: 0.64 - 23s 102ms/step - loss: 0.6802 - accuracy: 0.64 - 23s 101ms/step - loss: 0.6802 - accuracy: 0.64 - 23s 101ms/step - loss: 0.6796 - accuracy: 0.64 - 23s 101ms/step - loss: 0.6796 - accuracy: 0.64 - 23s 100ms/step - loss: 0.6788 - accuracy: 0.64 - 23s 100ms/step - loss: 0.6781 - accuracy: 0.64 - 23s 100ms/step - loss: 0.6771 - accuracy: 0.64 - 23s 99ms/step - loss: 0.6768 - accuracy: 0.6477 - 23s 99ms/step - loss: 0.6766 - accuracy: 0.648 - 23s 99ms/step - loss: 0.6756 - accuracy: 0.648 - 23s 99ms/step - loss: 0.6752 - accuracy: 0.649 - 23s 98ms/step - loss: 0.6751 - accuracy: 0.649 - 23s 98ms/step - loss: 0.6740 - accuracy: 0.649 - 23s 98ms/step - loss: 0.6742 - accuracy: 0.649 - 23s 97ms/step - loss: 0.6734 - accuracy: 0.650 - 23s 97ms/step - loss: 0.6732 - accuracy: 0.650 - 23s 97ms/step - loss: 0.6729 - accuracy: 0.650 - 23s 97ms/step - loss: 0.6722 - accuracy: 0.651 - 23s 96ms/step - loss: 0.6716 - accuracy: 0.651 - 23s 96ms/step - loss: 0.6712 - accuracy: 0.651 - 23s 96ms/step - loss: 0.6704 - accuracy: 0.652 - 23s 96ms/step - loss: 0.6701 - accuracy: 0.652 - 23s 95ms/step - loss: 0.6697 - accuracy: 0.652 - 23s 95ms/step - loss: 0.6695 - accuracy: 0.653 - 23s 95ms/step - loss: 0.6693 - accuracy: 0.653 - 23s 95ms/step - loss: 0.6687 - accuracy: 0.653 - 23s 94ms/step - loss: 0.6682 - accuracy: 0.654 - 23s 94ms/step - loss: 0.6678 - accuracy: 0.654 - 23s 94ms/step - loss: 0.6674 - accuracy: 0.654 - 24s 94ms/step - loss: 0.6669 - accuracy: 0.655 - 24s 93ms/step - loss: 0.6663 - accuracy: 0.655 - 24s 93ms/step - loss: 0.6654 - accuracy: 0.656 - 24s 93ms/step - loss: 0.6651 - accuracy: 0.656 - 24s 93ms/step - loss: 0.6647 - accuracy: 0.657 - 24s 92ms/step - loss: 0.6638 - accuracy: 0.657 - 24s 92ms/step - loss: 0.6635 - accuracy: 0.657 - 24s 92ms/step - loss: 0.6632 - accuracy: 0.657 - 24s 92ms/step - loss: 0.6622 - accuracy: 0.658 - 24s 92ms/step - loss: 0.6612 - accuracy: 0.659 - 24s 91ms/step - loss: 0.6614 - accuracy: 0.658 - 24s 91ms/step - loss: 0.6619 - accuracy: 0.658 - 24s 91ms/step - loss: 0.6617 - accuracy: 0.658 - 24s 91ms/step - loss: 0.6614 - accuracy: 0.659 - 24s 90ms/step - loss: 0.6611 - accuracy: 0.659 - 24s 90ms/step - loss: 0.6603 - accuracy: 0.659 - 24s 90ms/step - loss: 0.6602 - accuracy: 0.660 - 24s 90ms/step - loss: 0.6600 - accuracy: 0.660 - 24s 90ms/step - loss: 0.6598 - accuracy: 0.660 - 24s 89ms/step - loss: 0.6600 - accuracy: 0.660 - 24s 89ms/step - loss: 0.6596 - accuracy: 0.660 - 24s 89ms/step - loss: 0.6590 - accuracy: 0.661 - 24s 89ms/step - loss: 0.6583 - accuracy: 0.661 - 24s 89ms/step - loss: 0.6578 - accuracy: 0.661 - 24s 88ms/step - loss: 0.6573 - accuracy: 0.661 - 24s 88ms/step - loss: 0.6569 - accuracy: 0.662 - 24s 88ms/step - loss: 0.6562 - accuracy: 0.662 - 24s 88ms/step - loss: 0.6556 - accuracy: 0.662 - 24s 88ms/step - loss: 0.6550 - accuracy: 0.662 - 24s 87ms/step - loss: 0.6549 - accuracy: 0.662 - 25s 87ms/step - loss: 0.6543 - accuracy: 0.663 - 25s 87ms/step - loss: 0.6535 - accuracy: 0.663 - 25s 87ms/step - loss: 0.6529 - accuracy: 0.664 - 25s 87ms/step - loss: 0.6526 - accuracy: 0.664 - 25s 86ms/step - loss: 0.6522 - accuracy: 0.664 - 25s 86ms/step - loss: 0.6519 - accuracy: 0.664 - 25s 86ms/step - loss: 0.6516 - accuracy: 0.665 - 25s 86ms/step - loss: 0.6508 - accuracy: 0.665 - 25s 86ms/step - loss: 0.6506 - accuracy: 0.666 - 25s 86ms/step - loss: 0.6498 - accuracy: 0.666 - 25s 85ms/step - loss: 0.6494 - accuracy: 0.666 - 25s 85ms/step - loss: 0.6489 - accuracy: 0.667 - 25s 85ms/step - loss: 0.6492 - accuracy: 0.667 - 25s 85ms/step - loss: 0.6482 - accuracy: 0.668 - 25s 85ms/step - loss: 0.6476 - accuracy: 0.668 - 25s 84ms/step - loss: 0.6475 - accuracy: 0.668 - 25s 84ms/step - loss: 0.6469 - accuracy: 0.668 - 25s 84ms/step - loss: 0.6470 - accuracy: 0.668 - 25s 84ms/step - loss: 0.6472 - accuracy: 0.668 - 25s 84ms/step - loss: 0.6466 - accuracy: 0.669 - 25s 84ms/step - loss: 0.6463 - accuracy: 0.669 - 25s 83ms/step - loss: 0.6457 - accuracy: 0.669 - 25s 83ms/step - loss: 0.6454 - accuracy: 0.669 - 25s 83ms/step - loss: 0.6447 - accuracy: 0.670 - 25s 83ms/step - loss: 0.6442 - accuracy: 0.670 - 25s 83ms/step - loss: 0.6441 - accuracy: 0.671 - 25s 83ms/step - loss: 0.6433 - accuracy: 0.671 - 25s 82ms/step - loss: 0.6426 - accuracy: 0.671 - 25s 82ms/step - loss: 0.6424 - accuracy: 0.672 - 25s 82ms/step - loss: 0.6416 - accuracy: 0.672 - 25s 82ms/step - loss: 0.6410 - accuracy: 0.672 - 26s 82ms/step - loss: 0.6409 - accuracy: 0.673 - 26s 82ms/step - loss: 0.6403 - accuracy: 0.673 - 26s 81ms/step - loss: 0.6399 - accuracy: 0.674 - 26s 81ms/step - loss: 0.6391 - accuracy: 0.674 - 26s 81ms/step - loss: 0.6387 - accuracy: 0.674 - 26s 81ms/step - loss: 0.6381 - accuracy: 0.675 - 26s 81ms/step - loss: 0.6381 - accuracy: 0.675 - 26s 81ms/step - loss: 0.6375 - accuracy: 0.675 - 26s 81ms/step - loss: 0.6365 - accuracy: 0.676 - 26s 80ms/step - loss: 0.6365 - accuracy: 0.676 - 26s 80ms/step - loss: 0.6360 - accuracy: 0.676 - 26s 80ms/step - loss: 0.6355 - accuracy: 0.676 - 26s 80ms/step - loss: 0.6354 - accuracy: 0.677 - 26s 80ms/step - loss: 0.6352 - accuracy: 0.677 - 26s 80ms/step - loss: 0.6346 - accuracy: 0.677 - 26s 79ms/step - loss: 0.6342 - accuracy: 0.678 - 26s 79ms/step - loss: 0.6340 - accuracy: 0.678 - 26s 79ms/step - loss: 0.6334 - accuracy: 0.679 - 26s 79ms/step - loss: 0.6327 - accuracy: 0.679 - 26s 79ms/step - loss: 0.6323 - accuracy: 0.679 - 26s 79ms/step - loss: 0.6320 - accuracy: 0.680 - 26s 79ms/step - loss: 0.6319 - accuracy: 0.680 - 26s 79ms/step - loss: 0.6316 - accuracy: 0.680 - 26s 78ms/step - loss: 0.6312 - accuracy: 0.6809"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    502/Unknown - 26s 78ms/step - loss: 0.6312 - accuracy: 0.681 - 26s 78ms/step - loss: 0.6312 - accuracy: 0.681 - 26s 78ms/step - loss: 0.6308 - accuracy: 0.681 - 26s 78ms/step - loss: 0.6306 - accuracy: 0.681 - 26s 78ms/step - loss: 0.6301 - accuracy: 0.681 - 26s 78ms/step - loss: 0.6297 - accuracy: 0.682 - 26s 77ms/step - loss: 0.6292 - accuracy: 0.682 - 27s 77ms/step - loss: 0.6291 - accuracy: 0.682 - 27s 77ms/step - loss: 0.6287 - accuracy: 0.682 - 27s 77ms/step - loss: 0.6279 - accuracy: 0.683 - 27s 77ms/step - loss: 0.6278 - accuracy: 0.683 - 27s 77ms/step - loss: 0.6276 - accuracy: 0.683 - 27s 77ms/step - loss: 0.6272 - accuracy: 0.683 - 27s 77ms/step - loss: 0.6265 - accuracy: 0.684 - 27s 76ms/step - loss: 0.6261 - accuracy: 0.684 - 27s 76ms/step - loss: 0.6256 - accuracy: 0.684 - 27s 76ms/step - loss: 0.6251 - accuracy: 0.685 - 27s 76ms/step - loss: 0.6247 - accuracy: 0.685 - 27s 76ms/step - loss: 0.6242 - accuracy: 0.685 - 27s 76ms/step - loss: 0.6239 - accuracy: 0.685 - 27s 76ms/step - loss: 0.6234 - accuracy: 0.686 - 27s 76ms/step - loss: 0.6231 - accuracy: 0.686 - 27s 76ms/step - loss: 0.6228 - accuracy: 0.686 - 27s 75ms/step - loss: 0.6223 - accuracy: 0.686 - 27s 75ms/step - loss: 0.6222 - accuracy: 0.687 - 27s 75ms/step - loss: 0.6222 - accuracy: 0.687 - 27s 75ms/step - loss: 0.6216 - accuracy: 0.687 - 27s 75ms/step - loss: 0.6210 - accuracy: 0.687 - 27s 75ms/step - loss: 0.6205 - accuracy: 0.688 - 27s 75ms/step - loss: 0.6203 - accuracy: 0.688 - 27s 75ms/step - loss: 0.6201 - accuracy: 0.688 - 27s 74ms/step - loss: 0.6200 - accuracy: 0.688 - 27s 74ms/step - loss: 0.6199 - accuracy: 0.688 - 27s 74ms/step - loss: 0.6193 - accuracy: 0.688 - 27s 74ms/step - loss: 0.6189 - accuracy: 0.689 - 27s 74ms/step - loss: 0.6186 - accuracy: 0.689 - 27s 74ms/step - loss: 0.6188 - accuracy: 0.689 - 28s 74ms/step - loss: 0.6189 - accuracy: 0.689 - 28s 74ms/step - loss: 0.6186 - accuracy: 0.690 - 28s 74ms/step - loss: 0.6182 - accuracy: 0.690 - 28s 73ms/step - loss: 0.6178 - accuracy: 0.690 - 28s 73ms/step - loss: 0.6174 - accuracy: 0.690 - 28s 73ms/step - loss: 0.6174 - accuracy: 0.690 - 28s 73ms/step - loss: 0.6170 - accuracy: 0.691 - 28s 73ms/step - loss: 0.6170 - accuracy: 0.691 - 28s 73ms/step - loss: 0.6167 - accuracy: 0.691 - 28s 73ms/step - loss: 0.6163 - accuracy: 0.691 - 28s 73ms/step - loss: 0.6156 - accuracy: 0.692 - 28s 73ms/step - loss: 0.6151 - accuracy: 0.692 - 28s 73ms/step - loss: 0.6148 - accuracy: 0.692 - 28s 72ms/step - loss: 0.6144 - accuracy: 0.693 - 28s 72ms/step - loss: 0.6141 - accuracy: 0.693 - 28s 72ms/step - loss: 0.6137 - accuracy: 0.693 - 28s 72ms/step - loss: 0.6135 - accuracy: 0.693 - 28s 72ms/step - loss: 0.6131 - accuracy: 0.693 - 28s 72ms/step - loss: 0.6126 - accuracy: 0.694 - 28s 72ms/step - loss: 0.6121 - accuracy: 0.694 - 28s 72ms/step - loss: 0.6122 - accuracy: 0.694 - 28s 72ms/step - loss: 0.6119 - accuracy: 0.694 - 28s 72ms/step - loss: 0.6114 - accuracy: 0.694 - 28s 71ms/step - loss: 0.6114 - accuracy: 0.695 - 28s 71ms/step - loss: 0.6116 - accuracy: 0.695 - 28s 71ms/step - loss: 0.6111 - accuracy: 0.695 - 28s 71ms/step - loss: 0.6105 - accuracy: 0.695 - 28s 71ms/step - loss: 0.6103 - accuracy: 0.695 - 28s 71ms/step - loss: 0.6098 - accuracy: 0.696 - 28s 71ms/step - loss: 0.6092 - accuracy: 0.696 - 29s 71ms/step - loss: 0.6092 - accuracy: 0.696 - 29s 71ms/step - loss: 0.6089 - accuracy: 0.696 - 29s 71ms/step - loss: 0.6086 - accuracy: 0.697 - 29s 70ms/step - loss: 0.6082 - accuracy: 0.697 - 29s 70ms/step - loss: 0.6079 - accuracy: 0.697 - 29s 70ms/step - loss: 0.6075 - accuracy: 0.697 - 29s 70ms/step - loss: 0.6071 - accuracy: 0.698 - 29s 70ms/step - loss: 0.6070 - accuracy: 0.698 - 29s 70ms/step - loss: 0.6067 - accuracy: 0.698 - 29s 70ms/step - loss: 0.6062 - accuracy: 0.698 - 29s 70ms/step - loss: 0.6055 - accuracy: 0.698 - 29s 70ms/step - loss: 0.6052 - accuracy: 0.699 - 29s 70ms/step - loss: 0.6048 - accuracy: 0.699 - 29s 70ms/step - loss: 0.6045 - accuracy: 0.699 - 29s 69ms/step - loss: 0.6040 - accuracy: 0.699 - 29s 69ms/step - loss: 0.6036 - accuracy: 0.700 - 29s 69ms/step - loss: 0.6030 - accuracy: 0.700 - 29s 69ms/step - loss: 0.6027 - accuracy: 0.700 - 29s 69ms/step - loss: 0.6024 - accuracy: 0.700 - 29s 69ms/step - loss: 0.6019 - accuracy: 0.701 - 29s 69ms/step - loss: 0.6017 - accuracy: 0.701 - 29s 69ms/step - loss: 0.6013 - accuracy: 0.701 - 29s 69ms/step - loss: 0.6008 - accuracy: 0.701 - 29s 69ms/step - loss: 0.6005 - accuracy: 0.701 - 29s 69ms/step - loss: 0.5998 - accuracy: 0.702 - 29s 68ms/step - loss: 0.5993 - accuracy: 0.702 - 29s 68ms/step - loss: 0.5989 - accuracy: 0.703 - 29s 68ms/step - loss: 0.5986 - accuracy: 0.703 - 29s 68ms/step - loss: 0.5984 - accuracy: 0.703 - 29s 68ms/step - loss: 0.5981 - accuracy: 0.703 - 29s 68ms/step - loss: 0.5977 - accuracy: 0.703 - 29s 68ms/step - loss: 0.5977 - accuracy: 0.703 - 30s 68ms/step - loss: 0.5972 - accuracy: 0.704 - 30s 68ms/step - loss: 0.5968 - accuracy: 0.704 - 30s 68ms/step - loss: 0.5966 - accuracy: 0.704 - 30s 68ms/step - loss: 0.5963 - accuracy: 0.704 - 30s 67ms/step - loss: 0.5959 - accuracy: 0.704 - 30s 67ms/step - loss: 0.5954 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5953 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5951 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5948 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5948 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5944 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5940 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5937 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5936 - accuracy: 0.705 - 30s 67ms/step - loss: 0.5934 - accuracy: 0.706 - 30s 67ms/step - loss: 0.5934 - accuracy: 0.706 - 30s 67ms/step - loss: 0.5934 - accuracy: 0.706 - 30s 66ms/step - loss: 0.5931 - accuracy: 0.706 - 30s 66ms/step - loss: 0.5926 - accuracy: 0.706 - 30s 66ms/step - loss: 0.5925 - accuracy: 0.706 - 30s 66ms/step - loss: 0.5923 - accuracy: 0.706 - 30s 66ms/step - loss: 0.5926 - accuracy: 0.706 - 30s 66ms/step - loss: 0.5922 - accuracy: 0.707 - 30s 66ms/step - loss: 0.5919 - accuracy: 0.707 - 30s 66ms/step - loss: 0.5915 - accuracy: 0.707 - 30s 66ms/step - loss: 0.5915 - accuracy: 0.707 - 30s 66ms/step - loss: 0.5914 - accuracy: 0.707 - 30s 66ms/step - loss: 0.5908 - accuracy: 0.708 - 30s 66ms/step - loss: 0.5907 - accuracy: 0.708 - 30s 66ms/step - loss: 0.5905 - accuracy: 0.708 - 30s 65ms/step - loss: 0.5901 - accuracy: 0.708 - 30s 65ms/step - loss: 0.5901 - accuracy: 0.708 - 31s 65ms/step - loss: 0.5896 - accuracy: 0.708 - 31s 65ms/step - loss: 0.5895 - accuracy: 0.709 - 31s 65ms/step - loss: 0.5892 - accuracy: 0.709 - 31s 65ms/step - loss: 0.5887 - accuracy: 0.709 - 31s 65ms/step - loss: 0.5884 - accuracy: 0.709 - 31s 65ms/step - loss: 0.5883 - accuracy: 0.709 - 31s 65ms/step - loss: 0.5878 - accuracy: 0.710 - 31s 65ms/step - loss: 0.5873 - accuracy: 0.710 - 31s 65ms/step - loss: 0.5869 - accuracy: 0.710 - 31s 65ms/step - loss: 0.5869 - accuracy: 0.710 - 31s 65ms/step - loss: 0.5866 - accuracy: 0.710 - 31s 65ms/step - loss: 0.5866 - accuracy: 0.710 - 31s 64ms/step - loss: 0.5867 - accuracy: 0.711 - 31s 64ms/step - loss: 0.5864 - accuracy: 0.711 - 31s 64ms/step - loss: 0.5859 - accuracy: 0.711 - 31s 64ms/step - loss: 0.5858 - accuracy: 0.711 - 31s 64ms/step - loss: 0.5855 - accuracy: 0.711 - 31s 64ms/step - loss: 0.5853 - accuracy: 0.711 - 31s 64ms/step - loss: 0.5851 - accuracy: 0.711 - 31s 64ms/step - loss: 0.5850 - accuracy: 0.711 - 31s 64ms/step - loss: 0.5851 - accuracy: 0.712 - 31s 64ms/step - loss: 0.5848 - accuracy: 0.712 - 31s 64ms/step - loss: 0.5844 - accuracy: 0.712 - 31s 64ms/step - loss: 0.5843 - accuracy: 0.712 - 31s 64ms/step - loss: 0.5839 - accuracy: 0.712 - 31s 64ms/step - loss: 0.5838 - accuracy: 0.712 - 31s 63ms/step - loss: 0.5836 - accuracy: 0.712 - 31s 63ms/step - loss: 0.5835 - accuracy: 0.712 - 31s 63ms/step - loss: 0.5828 - accuracy: 0.713 - 31s 63ms/step - loss: 0.5823 - accuracy: 0.713 - 31s 63ms/step - loss: 0.5820 - accuracy: 0.713 - 31s 63ms/step - loss: 0.5815 - accuracy: 0.714 - 31s 63ms/step - loss: 0.5813 - accuracy: 0.714 - 32s 63ms/step - loss: 0.5810 - accuracy: 0.714 - 32s 63ms/step - loss: 0.5807 - accuracy: 0.714 - 32s 63ms/step - loss: 0.5805 - accuracy: 0.7146"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    669/Unknown - 32s 63ms/step - loss: 0.5803 - accuracy: 0.714 - 32s 63ms/step - loss: 0.5801 - accuracy: 0.715 - 32s 63ms/step - loss: 0.5796 - accuracy: 0.715 - 32s 63ms/step - loss: 0.5794 - accuracy: 0.715 - 32s 63ms/step - loss: 0.5790 - accuracy: 0.715 - 32s 63ms/step - loss: 0.5785 - accuracy: 0.715 - 32s 62ms/step - loss: 0.5783 - accuracy: 0.716 - 32s 62ms/step - loss: 0.5782 - accuracy: 0.716 - 32s 62ms/step - loss: 0.5783 - accuracy: 0.716 - 32s 62ms/step - loss: 0.5780 - accuracy: 0.716 - 32s 62ms/step - loss: 0.5779 - accuracy: 0.716 - 32s 62ms/step - loss: 0.5778 - accuracy: 0.716 - 32s 62ms/step - loss: 0.5775 - accuracy: 0.716 - 32s 62ms/step - loss: 0.5772 - accuracy: 0.717 - 32s 62ms/step - loss: 0.5767 - accuracy: 0.717 - 32s 62ms/step - loss: 0.5762 - accuracy: 0.717 - 32s 62ms/step - loss: 0.5761 - accuracy: 0.717 - 32s 62ms/step - loss: 0.5755 - accuracy: 0.718 - 32s 62ms/step - loss: 0.5752 - accuracy: 0.718 - 32s 62ms/step - loss: 0.5750 - accuracy: 0.718 - 32s 62ms/step - loss: 0.5746 - accuracy: 0.718 - 32s 62ms/step - loss: 0.5745 - accuracy: 0.718 - 32s 62ms/step - loss: 0.5743 - accuracy: 0.718 - 32s 61ms/step - loss: 0.5740 - accuracy: 0.719 - 32s 61ms/step - loss: 0.5739 - accuracy: 0.719 - 32s 61ms/step - loss: 0.5736 - accuracy: 0.719 - 32s 61ms/step - loss: 0.5734 - accuracy: 0.719 - 32s 61ms/step - loss: 0.5736 - accuracy: 0.719 - 32s 61ms/step - loss: 0.5734 - accuracy: 0.719 - 33s 61ms/step - loss: 0.5732 - accuracy: 0.719 - 33s 61ms/step - loss: 0.5729 - accuracy: 0.719 - 33s 61ms/step - loss: 0.5726 - accuracy: 0.719 - 33s 61ms/step - loss: 0.5723 - accuracy: 0.720 - 33s 61ms/step - loss: 0.5721 - accuracy: 0.720 - 33s 61ms/step - loss: 0.5718 - accuracy: 0.720 - 33s 61ms/step - loss: 0.5716 - accuracy: 0.720 - 33s 61ms/step - loss: 0.5713 - accuracy: 0.720 - 33s 61ms/step - loss: 0.5710 - accuracy: 0.721 - 33s 61ms/step - loss: 0.5707 - accuracy: 0.721 - 33s 61ms/step - loss: 0.5703 - accuracy: 0.721 - 33s 61ms/step - loss: 0.5701 - accuracy: 0.721 - 33s 60ms/step - loss: 0.5699 - accuracy: 0.721 - 33s 60ms/step - loss: 0.5697 - accuracy: 0.722 - 33s 60ms/step - loss: 0.5694 - accuracy: 0.722 - 33s 60ms/step - loss: 0.5691 - accuracy: 0.722 - 33s 60ms/step - loss: 0.5689 - accuracy: 0.722 - 33s 60ms/step - loss: 0.5687 - accuracy: 0.722 - 33s 60ms/step - loss: 0.5684 - accuracy: 0.722 - 33s 60ms/step - loss: 0.5683 - accuracy: 0.722 - 33s 60ms/step - loss: 0.5681 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5678 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5675 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5674 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5671 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5672 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5673 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5674 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5672 - accuracy: 0.723 - 33s 60ms/step - loss: 0.5669 - accuracy: 0.724 - 33s 59ms/step - loss: 0.5667 - accuracy: 0.724 - 33s 59ms/step - loss: 0.5664 - accuracy: 0.724 - 33s 59ms/step - loss: 0.5662 - accuracy: 0.724 - 34s 59ms/step - loss: 0.5659 - accuracy: 0.724 - 34s 59ms/step - loss: 0.5655 - accuracy: 0.724 - 34s 59ms/step - loss: 0.5653 - accuracy: 0.725 - 34s 59ms/step - loss: 0.5649 - accuracy: 0.725 - 34s 59ms/step - loss: 0.5646 - accuracy: 0.725 - 34s 59ms/step - loss: 0.5645 - accuracy: 0.725 - 34s 59ms/step - loss: 0.5642 - accuracy: 0.725 - 34s 59ms/step - loss: 0.5638 - accuracy: 0.725 - 34s 59ms/step - loss: 0.5634 - accuracy: 0.725 - 34s 59ms/step - loss: 0.5635 - accuracy: 0.725 - 34s 59ms/step - loss: 0.5633 - accuracy: 0.726 - 34s 59ms/step - loss: 0.5631 - accuracy: 0.726 - 34s 59ms/step - loss: 0.5628 - accuracy: 0.726 - 34s 59ms/step - loss: 0.5625 - accuracy: 0.726 - 34s 59ms/step - loss: 0.5623 - accuracy: 0.726 - 34s 59ms/step - loss: 0.5622 - accuracy: 0.726 - 34s 59ms/step - loss: 0.5620 - accuracy: 0.726 - 34s 59ms/step - loss: 0.5616 - accuracy: 0.726 - 34s 58ms/step - loss: 0.5618 - accuracy: 0.726 - 34s 58ms/step - loss: 0.5617 - accuracy: 0.727 - 34s 58ms/step - loss: 0.5616 - accuracy: 0.727 - 34s 58ms/step - loss: 0.5613 - accuracy: 0.727 - 34s 58ms/step - loss: 0.5607 - accuracy: 0.727 - 34s 58ms/step - loss: 0.5604 - accuracy: 0.727 - 34s 58ms/step - loss: 0.5602 - accuracy: 0.727 - 34s 58ms/step - loss: 0.5598 - accuracy: 0.728 - 34s 58ms/step - loss: 0.5595 - accuracy: 0.728 - 34s 58ms/step - loss: 0.5593 - accuracy: 0.728 - 34s 58ms/step - loss: 0.5591 - accuracy: 0.728 - 34s 58ms/step - loss: 0.5588 - accuracy: 0.728 - 34s 58ms/step - loss: 0.5589 - accuracy: 0.728 - 34s 58ms/step - loss: 0.5586 - accuracy: 0.728 - 35s 58ms/step - loss: 0.5583 - accuracy: 0.729 - 35s 58ms/step - loss: 0.5581 - accuracy: 0.729 - 35s 58ms/step - loss: 0.5581 - accuracy: 0.729 - 35s 58ms/step - loss: 0.5579 - accuracy: 0.729 - 35s 58ms/step - loss: 0.5581 - accuracy: 0.729 - 35s 58ms/step - loss: 0.5578 - accuracy: 0.729 - 35s 58ms/step - loss: 0.5577 - accuracy: 0.729 - 35s 57ms/step - loss: 0.5574 - accuracy: 0.729 - 35s 57ms/step - loss: 0.5572 - accuracy: 0.730 - 35s 57ms/step - loss: 0.5569 - accuracy: 0.730 - 35s 57ms/step - loss: 0.5566 - accuracy: 0.730 - 35s 57ms/step - loss: 0.5563 - accuracy: 0.730 - 35s 57ms/step - loss: 0.5561 - accuracy: 0.730 - 35s 57ms/step - loss: 0.5558 - accuracy: 0.730 - 35s 57ms/step - loss: 0.5557 - accuracy: 0.730 - 35s 57ms/step - loss: 0.5553 - accuracy: 0.731 - 35s 57ms/step - loss: 0.5551 - accuracy: 0.731 - 35s 57ms/step - loss: 0.5548 - accuracy: 0.731 - 35s 57ms/step - loss: 0.5548 - accuracy: 0.731 - 35s 57ms/step - loss: 0.5545 - accuracy: 0.731 - 35s 57ms/step - loss: 0.5543 - accuracy: 0.731 - 35s 57ms/step - loss: 0.5541 - accuracy: 0.732 - 35s 57ms/step - loss: 0.5539 - accuracy: 0.732 - 35s 57ms/step - loss: 0.5537 - accuracy: 0.732 - 35s 57ms/step - loss: 0.5533 - accuracy: 0.732 - 35s 57ms/step - loss: 0.5531 - accuracy: 0.732 - 35s 57ms/step - loss: 0.5530 - accuracy: 0.732 - 35s 57ms/step - loss: 0.5528 - accuracy: 0.732 - 35s 57ms/step - loss: 0.5526 - accuracy: 0.732 - 35s 57ms/step - loss: 0.5522 - accuracy: 0.733 - 35s 57ms/step - loss: 0.5521 - accuracy: 0.733 - 35s 56ms/step - loss: 0.5517 - accuracy: 0.733 - 36s 56ms/step - loss: 0.5516 - accuracy: 0.733 - 36s 56ms/step - loss: 0.5512 - accuracy: 0.733 - 36s 56ms/step - loss: 0.5513 - accuracy: 0.733 - 36s 56ms/step - loss: 0.5512 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5509 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5506 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5503 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5500 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5499 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5496 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5494 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5494 - accuracy: 0.734 - 36s 56ms/step - loss: 0.5493 - accuracy: 0.735 - 36s 56ms/step - loss: 0.5490 - accuracy: 0.735 - 36s 56ms/step - loss: 0.5487 - accuracy: 0.735 - 36s 56ms/step - loss: 0.5486 - accuracy: 0.735 - 36s 56ms/step - loss: 0.5484 - accuracy: 0.735 - 36s 56ms/step - loss: 0.5481 - accuracy: 0.735 - 36s 56ms/step - loss: 0.5478 - accuracy: 0.735 - 36s 56ms/step - loss: 0.5475 - accuracy: 0.735 - 36s 56ms/step - loss: 0.5474 - accuracy: 0.736 - 36s 56ms/step - loss: 0.5473 - accuracy: 0.736 - 36s 56ms/step - loss: 0.5470 - accuracy: 0.736 - 36s 56ms/step - loss: 0.5467 - accuracy: 0.736 - 36s 56ms/step - loss: 0.5465 - accuracy: 0.736 - 36s 55ms/step - loss: 0.5461 - accuracy: 0.736 - 36s 55ms/step - loss: 0.5458 - accuracy: 0.737 - 36s 55ms/step - loss: 0.5455 - accuracy: 0.737 - 36s 55ms/step - loss: 0.5452 - accuracy: 0.737 - 36s 55ms/step - loss: 0.5450 - accuracy: 0.737 - 36s 55ms/step - loss: 0.5448 - accuracy: 0.737 - 36s 55ms/step - loss: 0.5445 - accuracy: 0.737 - 36s 55ms/step - loss: 0.5442 - accuracy: 0.737 - 37s 55ms/step - loss: 0.5441 - accuracy: 0.737 - 37s 55ms/step - loss: 0.5439 - accuracy: 0.738 - 37s 55ms/step - loss: 0.5435 - accuracy: 0.738 - 37s 55ms/step - loss: 0.5434 - accuracy: 0.738 - 37s 55ms/step - loss: 0.5436 - accuracy: 0.738 - 37s 55ms/step - loss: 0.5432 - accuracy: 0.738 - 37s 55ms/step - loss: 0.5430 - accuracy: 0.738 - 37s 55ms/step - loss: 0.5427 - accuracy: 0.7387"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697/697 [==============================]0.5425 - accuracy: 0.738 - 37s 55ms/step - loss: 0.5424 - accuracy: 0.739 - 37s 55ms/step - loss: 0.5422 - accuracy: 0.739 - 37s 55ms/step - loss: 0.5421 - accuracy: 0.739 - 37s 55ms/step - loss: 0.5420 - accuracy: 0.739 - 37s 55ms/step - loss: 0.5417 - accuracy: 0.739 - 37s 55ms/step - loss: 0.5416 - accuracy: 0.739 - 37s 55ms/step - loss: 0.5414 - accuracy: 0.739 - 37s 55ms/step - loss: 0.5411 - accuracy: 0.739 - 37s 55ms/step - loss: 0.5410 - accuracy: 0.740 - 37s 55ms/step - loss: 0.5409 - accuracy: 0.740 - 37s 55ms/step - loss: 0.5406 - accuracy: 0.740 - 37s 54ms/step - loss: 0.5403 - accuracy: 0.740 - 37s 54ms/step - loss: 0.5401 - accuracy: 0.740 - 37s 54ms/step - loss: 0.5398 - accuracy: 0.740 - 37s 54ms/step - loss: 0.5397 - accuracy: 0.740 - 37s 54ms/step - loss: 0.5396 - accuracy: 0.740 - 37s 54ms/step - loss: 0.5393 - accuracy: 0.741 - 37s 54ms/step - loss: 0.5392 - accuracy: 0.741 - 37s 54ms/step - loss: 0.5389 - accuracy: 0.741 - 37s 54ms/step - loss: 0.5385 - accuracy: 0.741 - 37s 54ms/step - loss: 0.5385 - accuracy: 0.741 - 37s 54ms/step - loss: 0.5382 - accuracy: 0.741 - 38s 54ms/step - loss: 0.5380 - accuracy: 0.741 - 38s 54ms/step - loss: 0.5378 - accuracy: 0.741 - 38s 54ms/step - loss: 0.5377 - accuracy: 0.741 - 38s 54ms/step - loss: 0.5377 - accuracy: 0.742 - 38s 54ms/step - loss: 0.5375 - accuracy: 0.742 - 42s 60ms/step - loss: 0.5375 - accuracy: 0.7421 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "697/697 [==============================] ETA: 16:02 - loss: 0.3592 - accuracy: 0.859 - ETA: 5:14 - loss: 0.3654 - accuracy: 0.838 - ETA: 3:04 - loss: 0.3449 - accuracy: 0.85 - ETA: 2:08 - loss: 0.3087 - accuracy: 0.87 - ETA: 1:37 - loss: 0.3014 - accuracy: 0.87 - ETA: 1:18 - loss: 0.3154 - accuracy: 0.86 - ETA: 1:04 - loss: 0.3070 - accuracy: 0.87 - ETA: 54s - loss: 0.3236 - accuracy: 0.8687 - ETA: 46s - loss: 0.3283 - accuracy: 0.866 - ETA: 40s - loss: 0.3324 - accuracy: 0.865 - ETA: 35s - loss: 0.3240 - accuracy: 0.869 - ETA: 31s - loss: 0.3247 - accuracy: 0.870 - ETA: 28s - loss: 0.3252 - accuracy: 0.870 - ETA: 25s - loss: 0.3273 - accuracy: 0.869 - ETA: 22s - loss: 0.3278 - accuracy: 0.866 - ETA: 20s - loss: 0.3243 - accuracy: 0.869 - ETA: 18s - loss: 0.3175 - accuracy: 0.871 - ETA: 16s - loss: 0.3125 - accuracy: 0.874 - ETA: 15s - loss: 0.3129 - accuracy: 0.872 - ETA: 13s - loss: 0.3133 - accuracy: 0.873 - ETA: 12s - loss: 0.3110 - accuracy: 0.873 - ETA: 11s - loss: 0.3101 - accuracy: 0.873 - ETA: 10s - loss: 0.3119 - accuracy: 0.870 - ETA: 9s - loss: 0.3118 - accuracy: 0.870 - ETA: 8s - loss: 0.3131 - accuracy: 0.87 - ETA: 7s - loss: 0.3135 - accuracy: 0.87 - ETA: 6s - loss: 0.3139 - accuracy: 0.87 - ETA: 6s - loss: 0.3128 - accuracy: 0.87 - ETA: 5s - loss: 0.3160 - accuracy: 0.86 - ETA: 4s - loss: 0.3182 - accuracy: 0.86 - ETA: 4s - loss: 0.3173 - accuracy: 0.86 - ETA: 3s - loss: 0.3177 - accuracy: 0.86 - ETA: 3s - loss: 0.3190 - accuracy: 0.86 - ETA: 2s - loss: 0.3196 - accuracy: 0.86 - ETA: 2s - loss: 0.3191 - accuracy: 0.86 - ETA: 1s - loss: 0.3175 - accuracy: 0.86 - ETA: 1s - loss: 0.3152 - accuracy: 0.86 - ETA: 0s - loss: 0.3129 - accuracy: 0.86 - ETA: 0s - loss: 0.3110 - accuracy: 0.86 - 33s 48ms/step - loss: 0.3099 - accuracy: 0.8648 - val_loss: 0.3597 - val_accuracy: 0.8326\n",
      "Epoch 3/5\n",
      "697/697 [==============================] ETA: 15:09 - loss: 0.2495 - accuracy: 0.890 - ETA: 4:56 - loss: 0.2077 - accuracy: 0.916 - ETA: 2:54 - loss: 0.2083 - accuracy: 0.92 - ETA: 2:01 - loss: 0.2229 - accuracy: 0.91 - ETA: 1:32 - loss: 0.2246 - accuracy: 0.92 - ETA: 1:14 - loss: 0.2143 - accuracy: 0.92 - ETA: 1:01 - loss: 0.2011 - accuracy: 0.92 - ETA: 51s - loss: 0.2067 - accuracy: 0.9229 - ETA: 44s - loss: 0.2124 - accuracy: 0.919 - ETA: 38s - loss: 0.2055 - accuracy: 0.921 - ETA: 34s - loss: 0.2114 - accuracy: 0.919 - ETA: 30s - loss: 0.2122 - accuracy: 0.919 - ETA: 26s - loss: 0.2099 - accuracy: 0.918 - ETA: 24s - loss: 0.2121 - accuracy: 0.917 - ETA: 21s - loss: 0.2099 - accuracy: 0.917 - ETA: 19s - loss: 0.2096 - accuracy: 0.916 - ETA: 17s - loss: 0.2149 - accuracy: 0.913 - ETA: 15s - loss: 0.2109 - accuracy: 0.916 - ETA: 14s - loss: 0.2088 - accuracy: 0.916 - ETA: 13s - loss: 0.2091 - accuracy: 0.916 - ETA: 11s - loss: 0.2088 - accuracy: 0.916 - ETA: 10s - loss: 0.2119 - accuracy: 0.913 - ETA: 9s - loss: 0.2113 - accuracy: 0.913 - ETA: 8s - loss: 0.2135 - accuracy: 0.91 - ETA: 8s - loss: 0.2132 - accuracy: 0.91 - ETA: 7s - loss: 0.2118 - accuracy: 0.91 - ETA: 6s - loss: 0.2115 - accuracy: 0.91 - ETA: 5s - loss: 0.2118 - accuracy: 0.91 - ETA: 5s - loss: 0.2153 - accuracy: 0.91 - ETA: 4s - loss: 0.2149 - accuracy: 0.91 - ETA: 3s - loss: 0.2161 - accuracy: 0.90 - ETA: 3s - loss: 0.2173 - accuracy: 0.90 - ETA: 2s - loss: 0.2163 - accuracy: 0.90 - ETA: 2s - loss: 0.2149 - accuracy: 0.90 - ETA: 1s - loss: 0.2130 - accuracy: 0.90 - ETA: 1s - loss: 0.2130 - accuracy: 0.90 - ETA: 1s - loss: 0.2129 - accuracy: 0.90 - ETA: 0s - loss: 0.2145 - accuracy: 0.90 - ETA: 0s - loss: 0.2152 - accuracy: 0.90 - 33s 47ms/step - loss: 0.2308 - accuracy: 0.8979 - val_loss: 0.4064 - val_accuracy: 0.8348\n",
      "Epoch 4/5\n",
      "697/697 [==============================] ETA: 16:16 - loss: 0.1489 - accuracy: 0.921 - ETA: 5:18 - loss: 0.1218 - accuracy: 0.953 - ETA: 3:07 - loss: 0.1386 - accuracy: 0.94 - ETA: 2:10 - loss: 0.1461 - accuracy: 0.94 - ETA: 1:39 - loss: 0.1283 - accuracy: 0.94 - ETA: 1:19 - loss: 0.1274 - accuracy: 0.94 - ETA: 1:05 - loss: 0.1299 - accuracy: 0.94 - ETA: 55s - loss: 0.1369 - accuracy: 0.9448 - ETA: 47s - loss: 0.1344 - accuracy: 0.946 - ETA: 41s - loss: 0.1395 - accuracy: 0.941 - ETA: 36s - loss: 0.1415 - accuracy: 0.940 - ETA: 31s - loss: 0.1408 - accuracy: 0.940 - ETA: 28s - loss: 0.1448 - accuracy: 0.936 - ETA: 25s - loss: 0.1488 - accuracy: 0.936 - ETA: 22s - loss: 0.1529 - accuracy: 0.933 - ETA: 20s - loss: 0.1538 - accuracy: 0.932 - ETA: 18s - loss: 0.1522 - accuracy: 0.933 - ETA: 16s - loss: 0.1530 - accuracy: 0.931 - ETA: 15s - loss: 0.1526 - accuracy: 0.931 - ETA: 13s - loss: 0.1514 - accuracy: 0.931 - ETA: 12s - loss: 0.1552 - accuracy: 0.930 - ETA: 11s - loss: 0.1540 - accuracy: 0.931 - ETA: 10s - loss: 0.1542 - accuracy: 0.931 - ETA: 9s - loss: 0.1545 - accuracy: 0.931 - ETA: 8s - loss: 0.1563 - accuracy: 0.93 - ETA: 7s - loss: 0.1544 - accuracy: 0.93 - ETA: 6s - loss: 0.1531 - accuracy: 0.93 - ETA: 6s - loss: 0.1572 - accuracy: 0.93 - ETA: 5s - loss: 0.1565 - accuracy: 0.93 - ETA: 4s - loss: 0.1563 - accuracy: 0.93 - ETA: 4s - loss: 0.1553 - accuracy: 0.93 - ETA: 3s - loss: 0.1558 - accuracy: 0.93 - ETA: 3s - loss: 0.1581 - accuracy: 0.93 - ETA: 2s - loss: 0.1577 - accuracy: 0.93 - ETA: 2s - loss: 0.1573 - accuracy: 0.93 - ETA: 1s - loss: 0.1576 - accuracy: 0.93 - ETA: 1s - loss: 0.1559 - accuracy: 0.93 - ETA: 0s - loss: 0.1561 - accuracy: 0.93 - ETA: 0s - loss: 0.1574 - accuracy: 0.93 - 34s 49ms/step - loss: 0.1828 - accuracy: 0.9178 - val_loss: 0.4184 - val_accuracy: 0.8386\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697/697 [==============================] ETA: 16:31 - loss: 0.1005 - accuracy: 0.968 - ETA: 5:23 - loss: 0.1490 - accuracy: 0.937 - ETA: 3:09 - loss: 0.1396 - accuracy: 0.93 - ETA: 2:12 - loss: 0.1511 - accuracy: 0.91 - ETA: 1:40 - loss: 0.1408 - accuracy: 0.93 - ETA: 1:20 - loss: 0.1299 - accuracy: 0.93 - ETA: 1:06 - loss: 0.1304 - accuracy: 0.93 - ETA: 56s - loss: 0.1314 - accuracy: 0.9417 - ETA: 48s - loss: 0.1219 - accuracy: 0.945 - ETA: 41s - loss: 0.1150 - accuracy: 0.949 - ETA: 36s - loss: 0.1190 - accuracy: 0.949 - ETA: 32s - loss: 0.1226 - accuracy: 0.949 - ETA: 29s - loss: 0.1224 - accuracy: 0.948 - ETA: 25s - loss: 0.1206 - accuracy: 0.949 - ETA: 23s - loss: 0.1210 - accuracy: 0.949 - ETA: 21s - loss: 0.1225 - accuracy: 0.947 - ETA: 19s - loss: 0.1232 - accuracy: 0.947 - ETA: 17s - loss: 0.1216 - accuracy: 0.946 - ETA: 15s - loss: 0.1194 - accuracy: 0.947 - ETA: 14s - loss: 0.1201 - accuracy: 0.946 - ETA: 12s - loss: 0.1198 - accuracy: 0.946 - ETA: 11s - loss: 0.1195 - accuracy: 0.946 - ETA: 10s - loss: 0.1190 - accuracy: 0.945 - ETA: 9s - loss: 0.1172 - accuracy: 0.945 - ETA: 8s - loss: 0.1168 - accuracy: 0.94 - ETA: 7s - loss: 0.1192 - accuracy: 0.94 - ETA: 6s - loss: 0.1195 - accuracy: 0.94 - ETA: 6s - loss: 0.1182 - accuracy: 0.94 - ETA: 5s - loss: 0.1182 - accuracy: 0.94 - ETA: 5s - loss: 0.1185 - accuracy: 0.94 - ETA: 4s - loss: 0.1198 - accuracy: 0.94 - ETA: 3s - loss: 0.1217 - accuracy: 0.94 - ETA: 3s - loss: 0.1222 - accuracy: 0.94 - ETA: 2s - loss: 0.1226 - accuracy: 0.94 - ETA: 2s - loss: 0.1223 - accuracy: 0.94 - ETA: 1s - loss: 0.1229 - accuracy: 0.94 - ETA: 1s - loss: 0.1225 - accuracy: 0.94 - ETA: 1s - loss: 0.1236 - accuracy: 0.94 - ETA: 0s - loss: 0.1244 - accuracy: 0.94 - ETA: 0s - loss: 0.1257 - accuracy: 0.94 - 35s 50ms/step - loss: 0.1473 - accuracy: 0.9320 - val_loss: 0.4950 - val_accuracy: 0.8326\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_data,epochs=5,validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_25/embeddings:0' shape=(17179, 100) dtype=float32, numpy=\n",
       " array([[-0.09567659,  0.00166395, -0.1060098 , ...,  0.00327255,\n",
       "         -0.01806049,  0.04753019],\n",
       "        [-0.05738601,  0.14040187, -0.0940075 , ..., -0.09722354,\n",
       "         -0.11437219,  0.09325634],\n",
       "        [ 0.0654543 , -0.04119615,  0.05365785, ...,  0.04737078,\n",
       "          0.08713071, -0.04407173],\n",
       "        ...,\n",
       "        [-0.07416567,  0.14865762, -0.09810467, ..., -0.09540524,\n",
       "         -0.11657237,  0.09974745],\n",
       "        [-0.00858533,  0.0738495 , -0.06046059, ..., -0.07151202,\n",
       "         -0.06736542,  0.03489744],\n",
       "        [ 0.03109111, -0.04099759,  0.02248403, ..., -0.00060716,\n",
       "          0.05503817, -0.05373425]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_24/forward_lstm_24/kernel:0' shape=(100, 256) dtype=float32, numpy=\n",
       " array([[ 0.10242917, -0.0090631 , -0.11622679, ...,  0.02952322,\n",
       "         -0.02083237, -0.00182961],\n",
       "        [ 0.02337981, -0.14690922, -0.16672744, ..., -0.03077628,\n",
       "         -0.035759  , -0.02354477],\n",
       "        [ 0.05476   , -0.02612294, -0.09638835, ..., -0.10556342,\n",
       "          0.13120326, -0.18017656],\n",
       "        ...,\n",
       "        [ 0.09325793, -0.12878156, -0.22083127, ..., -0.16426092,\n",
       "          0.07666797, -0.02941863],\n",
       "        [ 0.16755913,  0.15835215, -0.14807492, ...,  0.0334973 ,\n",
       "          0.20199887, -0.08794066],\n",
       "        [-0.05009117, -0.12615736,  0.21923308, ..., -0.18133387,\n",
       "         -0.03029136,  0.06854467]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_24/forward_lstm_24/recurrent_kernel:0' shape=(64, 256) dtype=float32, numpy=\n",
       " array([[-0.11994497,  0.06409006, -0.00889282, ...,  0.11317184,\n",
       "         -0.16540246,  0.10515714],\n",
       "        [ 0.04230311,  0.05038588,  0.05851642, ..., -0.00828225,\n",
       "          0.10021426, -0.10878544],\n",
       "        [ 0.15018706, -0.01887191,  0.10762698, ...,  0.0188282 ,\n",
       "          0.11364224, -0.1635329 ],\n",
       "        ...,\n",
       "        [ 0.0456311 , -0.11177325,  0.01292125, ...,  0.01525702,\n",
       "         -0.02036924,  0.10525537],\n",
       "        [ 0.02060853,  0.07826192, -0.11463934, ..., -0.01871316,\n",
       "          0.00748039,  0.06742686],\n",
       "        [-0.04161852, -0.0207199 ,  0.04718188, ..., -0.05582582,\n",
       "          0.06473114,  0.14774314]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_24/forward_lstm_24/bias:0' shape=(256,) dtype=float32, numpy=\n",
       " array([-7.86557049e-02, -5.23010790e-02,  2.33110450e-02, -3.64453793e-02,\n",
       "        -2.44173966e-02, -4.60972190e-02,  5.52169699e-03, -3.41286622e-02,\n",
       "         7.26791890e-03, -3.84529904e-02, -1.66348182e-02, -3.00872922e-02,\n",
       "        -2.94244066e-02,  4.73054238e-02, -5.69848865e-02,  7.83086866e-02,\n",
       "         4.26428951e-02, -3.35602574e-02, -4.95464541e-02, -2.61303093e-02,\n",
       "        -5.68928868e-02, -1.23515632e-02,  6.82976842e-03,  2.68709306e-02,\n",
       "         1.68849956e-02, -5.39369248e-02, -1.08420420e-02, -8.26728269e-02,\n",
       "         8.71129557e-02,  3.58850841e-04, -5.40144555e-02, -4.20216843e-02,\n",
       "         1.88197866e-02, -2.88096052e-02, -5.42434789e-02, -2.12459471e-02,\n",
       "         1.78184290e-03,  1.01119028e-02, -5.01892017e-03, -2.83890087e-02,\n",
       "        -1.06734503e-02,  6.77375356e-03, -4.21827286e-02, -1.52896834e-03,\n",
       "         8.86795670e-02, -4.31604162e-02, -3.51253003e-02, -5.85955335e-03,\n",
       "        -1.95058193e-02,  3.35818715e-02, -1.94450282e-02, -4.71399017e-02,\n",
       "        -4.91426140e-02, -9.13063996e-03, -3.76840904e-02, -5.15467264e-02,\n",
       "        -4.42766063e-02, -2.09320821e-02,  4.61660400e-02, -5.40738143e-02,\n",
       "        -4.02517021e-02, -3.32728885e-02, -2.01402809e-02, -8.45747627e-03,\n",
       "         9.16049778e-01,  1.00107861e+00,  1.09243655e+00,  1.01556993e+00,\n",
       "         1.01415193e+00,  1.03845406e+00,  1.05204093e+00,  9.80704248e-01,\n",
       "         1.03043616e+00,  1.00682533e+00,  1.04944229e+00,  1.01570165e+00,\n",
       "         1.01938963e+00,  1.04557312e+00,  9.73113120e-01,  1.16402531e+00,\n",
       "         1.12508237e+00,  1.00980747e+00,  1.01012921e+00,  9.93072927e-01,\n",
       "         9.83980358e-01,  1.02774489e+00,  1.05085468e+00,  1.12838221e+00,\n",
       "         1.04268503e+00,  1.00136244e+00,  1.04365098e+00,  9.62374985e-01,\n",
       "         1.18082857e+00,  1.02812350e+00,  9.99096572e-01,  9.80531037e-01,\n",
       "         1.10368848e+00,  1.02608442e+00,  9.64426756e-01,  1.01562786e+00,\n",
       "         1.04809594e+00,  1.08053982e+00,  1.04539287e+00,  1.01345134e+00,\n",
       "         1.01689112e+00,  1.04427886e+00,  9.90710378e-01,  1.12671781e+00,\n",
       "         1.15347290e+00,  1.00937760e+00,  1.00619113e+00,  1.03346062e+00,\n",
       "         9.88367736e-01,  1.11044788e+00,  1.01636517e+00,  9.76229310e-01,\n",
       "         9.94444132e-01,  1.04385352e+00,  9.90210593e-01,  9.96782362e-01,\n",
       "         9.93986785e-01,  1.01805782e+00,  1.07138562e+00,  9.98237729e-01,\n",
       "         1.02696311e+00,  9.95774508e-01,  1.04021311e+00,  1.05180097e+00,\n",
       "        -1.29959565e-02, -1.49476919e-02, -1.15360208e-02, -1.45324701e-02,\n",
       "        -7.84578454e-03,  4.56639081e-02,  2.78203916e-02,  1.59013160e-02,\n",
       "        -2.76417676e-02, -1.90876555e-02, -1.02765579e-02,  1.04926322e-02,\n",
       "         1.08937677e-02,  9.07293521e-03,  2.06745975e-02, -7.16491044e-02,\n",
       "         3.19027528e-02, -2.32401546e-02,  3.10615432e-02,  1.46578543e-03,\n",
       "         2.10065618e-02,  1.70048028e-02,  1.75909735e-02, -6.49653226e-02,\n",
       "         3.72404046e-02,  1.72009338e-02,  1.39872814e-02, -2.01257970e-02,\n",
       "         3.24595273e-02,  2.02501994e-02,  2.21805647e-02, -1.21081539e-03,\n",
       "         4.18807045e-02, -7.24610034e-03,  2.55963132e-02, -6.61966484e-03,\n",
       "        -2.92550623e-02,  7.81549327e-03, -3.46943882e-04,  1.20423408e-02,\n",
       "        -8.47929157e-03,  2.73391884e-03, -3.20701138e-03,  6.37184903e-02,\n",
       "         9.91983153e-03, -1.84993967e-02, -2.24835612e-02, -2.13852637e-02,\n",
       "        -1.82107314e-02, -5.38206771e-02, -1.29869152e-02, -6.55735610e-03,\n",
       "         5.40894270e-03,  9.73183010e-03, -6.73465990e-03,  1.62921881e-03,\n",
       "         8.02609231e-03,  1.61428616e-04,  4.65773838e-03,  8.44283088e-04,\n",
       "        -2.51161773e-02, -6.63107866e-03, -2.74622086e-02,  3.30387731e-03,\n",
       "        -8.53874311e-02, -4.01329100e-02,  1.48939807e-02, -4.55764942e-02,\n",
       "        -2.86240503e-02, -1.99425388e-02,  1.62438955e-02, -1.92373786e-02,\n",
       "         3.99798388e-03, -2.71016136e-02, -4.43053618e-03, -1.34619595e-02,\n",
       "        -3.09211407e-02,  7.19956830e-02, -4.26240265e-02,  1.08071782e-01,\n",
       "         2.74179094e-02,  1.09810814e-01, -6.39680177e-02, -1.64134502e-02,\n",
       "        -4.61552404e-02, -2.45135911e-02,  6.22597430e-03,  2.93384753e-02,\n",
       "         1.83277056e-02, -4.86408100e-02, -2.10421812e-02, -6.89379796e-02,\n",
       "         8.81953761e-02, -1.03278877e-02, -5.70474938e-02, -5.08124642e-02,\n",
       "         8.39074031e-02, -2.01490764e-02, -5.54839633e-02, -2.75278650e-02,\n",
       "         5.15715545e-03, -3.67157185e-03, -9.43286996e-03, -1.33200502e-02,\n",
       "        -7.24194618e-03, -1.52430276e-03, -4.50560190e-02,  1.22075314e-02,\n",
       "         8.35966617e-02, -3.04862019e-02, -3.98471877e-02, -1.02120657e-02,\n",
       "        -1.28398621e-02,  5.38321473e-02, -1.71290748e-02, -2.83864569e-02,\n",
       "        -4.68257554e-02, -1.57797094e-02, -1.03793452e-02, -4.84677963e-02,\n",
       "        -3.65950838e-02, -2.69401968e-02,  7.49278218e-02, -2.69248653e-02,\n",
       "        -5.63273653e-02, -2.36307178e-02, -2.27809865e-02,  7.77535746e-03],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_24/backward_lstm_24/kernel:0' shape=(100, 256) dtype=float32, numpy=\n",
       " array([[ 0.1680638 ,  0.16631626,  0.14383247, ...,  0.07537762,\n",
       "          0.11649051,  0.04814036],\n",
       "        [ 0.13279387,  0.04121425,  0.10225913, ..., -0.09804628,\n",
       "          0.04421138,  0.13732249],\n",
       "        [ 0.07188053, -0.12068807,  0.09967339, ...,  0.02406726,\n",
       "          0.08876013,  0.09617992],\n",
       "        ...,\n",
       "        [-0.17389922, -0.02851579, -0.04014211, ..., -0.2815781 ,\n",
       "         -0.25646198, -0.31981948],\n",
       "        [-0.2014095 , -0.11828323,  0.01527361, ..., -0.06476919,\n",
       "          0.08606762, -0.28565404],\n",
       "        [ 0.08565854,  0.10631514, -0.08504766, ..., -0.06266265,\n",
       "          0.01065241, -0.08464997]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_24/backward_lstm_24/recurrent_kernel:0' shape=(64, 256) dtype=float32, numpy=\n",
       " array([[-0.04006067, -0.08097769, -0.00045451, ...,  0.04481295,\n",
       "         -0.01579294, -0.01115313],\n",
       "        [ 0.00319045,  0.02178214,  0.00608858, ...,  0.07070912,\n",
       "          0.06773049,  0.08600801],\n",
       "        [ 0.12669058,  0.07648183, -0.00571483, ...,  0.08093351,\n",
       "          0.11405795, -0.06847062],\n",
       "        ...,\n",
       "        [ 0.05524458, -0.0184929 , -0.05481669, ..., -0.00508652,\n",
       "         -0.05994633, -0.07141766],\n",
       "        [-0.0564021 , -0.0217764 ,  0.02084192, ..., -0.0943144 ,\n",
       "         -0.1171907 , -0.06334502],\n",
       "        [-0.11889765, -0.05685291, -0.13340226, ...,  0.03161541,\n",
       "         -0.041109  , -0.02340228]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_24/backward_lstm_24/bias:0' shape=(256,) dtype=float32, numpy=\n",
       " array([-1.95206776e-02,  1.65315289e-02, -1.09120309e-02,  1.17679328e-01,\n",
       "        -1.30700907e-02, -1.05608590e-01, -7.98996072e-03,  1.81066133e-02,\n",
       "        -1.00471862e-02,  6.26550168e-02, -3.52224335e-02, -7.05179805e-03,\n",
       "        -3.97594720e-02,  3.40947555e-03, -6.65518865e-02, -2.70384084e-02,\n",
       "        -1.74162686e-02,  1.50364004e-02, -3.79035925e-03, -1.68864466e-02,\n",
       "        -1.00672990e-02, -3.19717079e-02, -9.27817356e-03, -3.20039280e-02,\n",
       "        -9.28443298e-03,  5.10491841e-02, -3.41206379e-02,  5.07307798e-03,\n",
       "         3.35720032e-02,  2.12423056e-02, -7.28324335e-03, -1.26172258e-02,\n",
       "        -5.58720492e-02,  1.73304230e-03,  3.31402346e-02, -1.22553729e-01,\n",
       "        -6.06373958e-02,  2.93952376e-02, -3.91556323e-02, -3.36757638e-02,\n",
       "        -6.89019039e-02,  5.41144190e-03,  5.81822544e-02, -1.40932035e-02,\n",
       "        -3.61757539e-02,  5.70797827e-03,  5.87843452e-03,  6.15952443e-03,\n",
       "         5.65231964e-03, -3.05495709e-02, -3.76856849e-02, -3.09042807e-04,\n",
       "        -6.89218752e-03, -9.75828618e-02, -2.96366811e-02, -4.99675870e-02,\n",
       "        -2.21200883e-02, -3.54607329e-02,  6.92904517e-02, -3.24569829e-02,\n",
       "         4.58798222e-02, -6.46536723e-02,  5.99279488e-03, -6.02075271e-02,\n",
       "         9.96262670e-01,  1.02930951e+00,  1.00328910e+00,  1.05698097e+00,\n",
       "         1.00332499e+00,  1.11088407e+00,  9.97819424e-01,  1.02923977e+00,\n",
       "         9.90533054e-01,  1.07571721e+00,  9.74551141e-01,  1.00392759e+00,\n",
       "         9.73119438e-01,  1.01198924e+00,  9.66800690e-01,  9.96722639e-01,\n",
       "         9.96698201e-01,  1.04594123e+00,  1.00551271e+00,  1.00100183e+00,\n",
       "         9.94754851e-01,  9.72510278e-01,  1.00151598e+00,  9.82936144e-01,\n",
       "         1.00040507e+00,  1.08888674e+00,  9.82309163e-01,  1.00647902e+00,\n",
       "         1.03228009e+00,  1.03461587e+00,  1.04112244e+00,  1.00404513e+00,\n",
       "         9.69111860e-01,  1.01247442e+00,  1.05268991e+00,  8.99873793e-01,\n",
       "         9.74854052e-01,  1.04877257e+00,  9.74951148e-01,  9.77558374e-01,\n",
       "         9.29524899e-01,  1.02064514e+00,  1.06441510e+00,  1.00240910e+00,\n",
       "         9.74137008e-01,  1.01969850e+00,  1.02622199e+00,  1.03763676e+00,\n",
       "         1.03382826e+00,  9.55635071e-01,  9.91471529e-01,  1.01571584e+00,\n",
       "         1.00346744e+00,  9.57314074e-01,  9.85614240e-01,  9.54416156e-01,\n",
       "         9.64894116e-01,  9.71173882e-01,  1.00677550e+00,  9.82255757e-01,\n",
       "         1.03919744e+00,  9.63340342e-01,  1.02491522e+00,  9.49103296e-01,\n",
       "        -3.96333449e-02,  2.33012885e-02,  3.07949372e-02,  5.87654486e-02,\n",
       "        -2.55748294e-02, -2.74916906e-02,  4.38198447e-02, -1.29001904e-02,\n",
       "        -1.86414588e-02,  5.17500378e-02,  1.77539662e-02,  2.05354337e-02,\n",
       "         1.86255705e-02, -2.86648367e-02, -3.69303562e-02,  2.40220632e-02,\n",
       "         1.97611190e-02,  3.22200172e-02, -1.31659815e-02,  3.40327621e-02,\n",
       "        -2.07626540e-02,  2.25162078e-02,  3.11251413e-02, -1.24790547e-02,\n",
       "        -1.37013122e-02, -5.36398850e-02, -4.95086843e-03, -2.93087009e-02,\n",
       "        -5.67289144e-02,  2.12277728e-03, -2.25031120e-03, -3.53966616e-02,\n",
       "         8.69527366e-03,  3.05405818e-02,  7.36201275e-03,  2.76956055e-03,\n",
       "        -2.58776452e-02,  1.76781565e-02,  1.88194793e-02,  2.39218026e-02,\n",
       "         2.73814648e-02, -2.03328114e-02, -5.81155121e-02, -2.59562824e-02,\n",
       "        -4.84606065e-02, -1.26030641e-02, -1.18468730e-02, -4.69601527e-02,\n",
       "         1.18029607e-03,  1.13290772e-02, -4.88028526e-02,  7.57475290e-03,\n",
       "         1.10755432e-02,  1.54375238e-03, -2.14866214e-02,  3.63998562e-02,\n",
       "        -2.49138810e-02, -8.58804910e-04, -5.01009300e-02,  2.31341776e-02,\n",
       "        -5.32292901e-03, -2.47705784e-02, -1.04451552e-02, -3.15414108e-02,\n",
       "        -2.51674280e-02, -2.98823556e-03, -1.31686125e-02,  1.19404130e-01,\n",
       "        -1.32098310e-02,  7.35191181e-02, -1.46430405e-02,  8.12048092e-03,\n",
       "        -2.70914081e-02,  7.20216334e-02, -3.67751680e-02, -1.41751124e-02,\n",
       "        -4.14535068e-02, -1.30341128e-02, -4.98953499e-02, -2.69960184e-02,\n",
       "        -2.39881072e-02,  9.63415205e-03, -1.51221901e-02, -1.96551159e-02,\n",
       "        -1.85675249e-02, -3.35329771e-02, -2.02003550e-02, -3.95635441e-02,\n",
       "        -1.97089352e-02,  4.63148393e-02, -3.20354141e-02, -7.33669987e-03,\n",
       "         3.76154520e-02,  3.51500674e-03, -2.01090127e-02, -7.06724077e-03,\n",
       "        -6.16003051e-02, -1.28363696e-04,  6.24096487e-03, -1.18198492e-01,\n",
       "        -6.76054582e-02,  2.82464828e-03, -3.43887471e-02, -3.53009738e-02,\n",
       "        -6.84538633e-02, -3.63040389e-03,  5.47786057e-02, -2.15152632e-02,\n",
       "        -3.60537432e-02, -1.40856113e-02, -1.26961609e-02,  1.13634737e-02,\n",
       "        -1.64042450e-02, -3.26752402e-02, -3.73567119e-02, -3.01701622e-03,\n",
       "        -1.57981943e-02, -1.13088645e-01, -3.24059986e-02, -4.52288687e-02,\n",
       "        -2.69798040e-02, -3.77541147e-02,  8.75590742e-02, -3.51340584e-02,\n",
       "         2.97084451e-02, -6.72262982e-02, -1.07736494e-02, -5.84743172e-02],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'attention_layer_19/kernel1:0' shape=(128, 50) dtype=float32, numpy=\n",
       " array([[ 2.20693299e-12, -1.22698011e-12, -1.99614240e-12, ...,\n",
       "          6.31885698e-14, -6.27376536e-12, -8.18720796e-13],\n",
       "        [-2.75925593e-12, -3.43970338e-12,  1.50045883e-12, ...,\n",
       "          1.57986078e-14,  1.11111988e-11,  1.37271271e-11],\n",
       "        [-1.59015607e-12, -1.07444817e-11, -5.06454297e-12, ...,\n",
       "          3.43696398e-13,  8.67014360e-13, -9.57340977e-12],\n",
       "        ...,\n",
       "        [ 7.14303660e-12,  4.89589966e-12, -1.83534806e-12, ...,\n",
       "          9.56793442e-14, -3.94275454e-12,  2.67819100e-12],\n",
       "        [ 3.18439697e-12,  3.64724154e-11, -1.79505577e-11, ...,\n",
       "         -2.90185389e-12, -1.69384951e-10, -1.03286935e-10],\n",
       "        [ 7.89441255e-12,  1.46448288e-11, -4.80865590e-11, ...,\n",
       "          1.04293317e-13, -7.86027979e-11, -1.64677022e-11]], dtype=float32)>,\n",
       " <tf.Variable 'attention_layer_19/bias1:0' shape=(1, 50) dtype=float32, numpy=\n",
       " array([[ 0.0085157 ,  0.013446  , -0.01341947,  0.00884877, -0.00263421,\n",
       "         -0.01419383,  0.00870186, -0.03092742,  0.03582685,  0.00824756,\n",
       "         -0.00339806, -0.01562008, -0.00144868, -0.00296896,  0.04464506,\n",
       "         -0.01211805, -0.00897807, -0.00912424, -0.0009651 ,  0.04582456,\n",
       "          0.02638083,  0.01935395,  0.03861634, -0.01756882,  0.00763319,\n",
       "          0.03006946, -0.00941371, -0.04226597,  0.01850658, -0.00282589,\n",
       "          0.0087118 , -0.00613259,  0.01125877,  0.02585544,  0.01064878,\n",
       "         -0.00362413, -0.00877631, -0.02463701, -0.0132096 , -0.01476776,\n",
       "         -0.03035853,  0.02676491, -0.01054524, -0.02565027,  0.02222716,\n",
       "          0.00675404,  0.02513745, -0.00291669, -0.03240342, -0.01112496]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'attention_layer_19/kernel2:0' shape=(50, 3) dtype=float32, numpy=\n",
       " array([[-7.18785587e-11, -1.32320641e-10,  2.52344590e-10],\n",
       "        [-9.88431767e-11, -1.03679648e-10,  1.49974838e-10],\n",
       "        [ 1.92857424e-10,  1.28980313e-10,  9.27037405e-11],\n",
       "        [-1.90830338e-10, -1.48332721e-10,  3.16370402e-11],\n",
       "        [-1.60985981e-11,  2.80990821e-11,  2.34548990e-12],\n",
       "        [ 1.54573923e-10,  2.19128340e-10, -1.58474428e-10],\n",
       "        [-2.69160891e-11, -9.75751008e-11, -1.72145180e-11],\n",
       "        [ 7.00849989e-10,  4.81069518e-10, -7.53125062e-10],\n",
       "        [-4.24299706e-10, -5.31203581e-10,  2.37624531e-10],\n",
       "        [-1.74378456e-10, -6.71932995e-11,  3.02835361e-11],\n",
       "        [-1.40731946e-12,  3.05940030e-11, -1.60547183e-11],\n",
       "        [ 3.82332999e-10,  3.64681646e-10, -2.09380638e-10],\n",
       "        [ 3.03717056e-12,  2.86339112e-11, -5.80595424e-12],\n",
       "        [ 9.23274217e-12,  1.99381678e-10, -3.44913117e-12],\n",
       "        [-8.08335843e-10, -8.53468518e-10,  2.31455730e-10],\n",
       "        [ 1.37575867e-10,  7.31448901e-10, -3.70400127e-10],\n",
       "        [ 2.18410040e-10,  1.40507148e-10,  3.03406987e-11],\n",
       "        [ 3.20869026e-11,  1.32525435e-10, -9.60003466e-11],\n",
       "        [-1.68100213e-12,  1.66497025e-11, -2.93334129e-12],\n",
       "        [-4.05851353e-10, -5.17792254e-10,  2.09069928e-10],\n",
       "        [-5.80205495e-10, -3.68956393e-10,  4.07605588e-10],\n",
       "        [-1.76967468e-11, -4.29317082e-10,  4.72496660e-11],\n",
       "        [ 2.16695412e-12, -3.93259231e-10,  5.88733506e-10],\n",
       "        [ 2.59767034e-11,  2.37915410e-10, -6.17260854e-10],\n",
       "        [-4.04993400e-11, -2.25350155e-10, -7.17238335e-13],\n",
       "        [-2.12110385e-10, -4.22252705e-10,  6.39306941e-11],\n",
       "        [-2.19551998e-11,  3.00633546e-10, -1.87561883e-10],\n",
       "        [ 6.61217303e-10,  6.42283615e-10, -1.62926103e-10],\n",
       "        [ 2.30097209e-11, -3.53455737e-10, -4.38179007e-13],\n",
       "        [ 3.08830739e-12,  5.42052549e-11, -7.92276592e-11],\n",
       "        [-5.87369944e-11, -1.43110454e-10,  1.83335652e-10],\n",
       "        [ 7.25953238e-12,  1.35374448e-10, -6.96327440e-11],\n",
       "        [ 1.64045964e-11, -1.19378632e-10,  5.91883834e-11],\n",
       "        [-1.42464582e-10, -3.31992378e-10,  2.21228785e-10],\n",
       "        [-1.19422042e-11, -1.06742740e-10, -1.72936075e-11],\n",
       "        [ 1.33043923e-11,  6.10044168e-11, -1.29761254e-11],\n",
       "        [ 1.02816949e-10,  1.20113225e-10,  1.64056789e-11],\n",
       "        [ 1.79925810e-10,  3.47678220e-10, -8.15672807e-11],\n",
       "        [ 1.47327442e-10,  1.87210220e-10, -6.89490062e-11],\n",
       "        [ 3.52005980e-10,  1.83299209e-10, -1.46193280e-10],\n",
       "        [ 4.25085495e-10,  1.11600773e-09, -1.35535957e-10],\n",
       "        [-3.21151550e-10, -3.16517174e-10,  1.75893897e-10],\n",
       "        [ 1.41083784e-10,  9.08771738e-11,  8.86340654e-12],\n",
       "        [ 1.88872432e-10,  4.87357821e-10, -8.52552542e-11],\n",
       "        [-2.14790005e-10, -1.15792355e-10,  9.56674531e-11],\n",
       "        [-1.31024549e-11, -4.08641759e-10,  3.29508781e-11],\n",
       "        [-1.26731764e-10, -8.38714542e-10,  7.56838037e-10],\n",
       "        [ 2.76274403e-12,  1.82563027e-11, -1.86762932e-11],\n",
       "        [ 4.26853886e-10,  7.65407626e-10, -2.40786391e-10],\n",
       "        [-6.96208091e-11,  9.22327104e-10, -5.62366750e-11]], dtype=float32)>,\n",
       " <tf.Variable 'attention_layer_19/bias2:0' shape=(1, 3) dtype=float32, numpy=array([[-0.00035139, -0.00108676, -0.00037577]], dtype=float32)>,\n",
       " <tf.Variable 'dense_48/kernel:0' shape=(384, 64) dtype=float32, numpy=\n",
       " array([[-0.06116375, -0.09997822, -0.09034763, ..., -0.03749066,\n",
       "         -0.03736566,  0.06990391],\n",
       "        [-0.1280079 , -0.14067948,  0.03410685, ...,  0.04882307,\n",
       "         -0.16904566,  0.07244688],\n",
       "        [ 0.05012864,  0.06825324,  0.05312923, ..., -0.17118128,\n",
       "         -0.00272237, -0.02236024],\n",
       "        ...,\n",
       "        [ 0.05848808, -0.01267532,  0.01411706, ..., -0.00413328,\n",
       "         -0.10637514, -0.03568379],\n",
       "        [-0.06617963, -0.05807782, -0.13599345, ..., -0.05644678,\n",
       "          0.10348584,  0.01572081],\n",
       "        [ 0.03619475, -0.07866517, -0.03900578, ..., -0.12776731,\n",
       "         -0.11173695, -0.06257665]], dtype=float32)>,\n",
       " <tf.Variable 'dense_48/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([-0.00503564, -0.0075365 , -0.06131855,  0.0419656 , -0.07367761,\n",
       "         0.00421904,  0.03524809, -0.02101885,  0.05607025, -0.04600957,\n",
       "        -0.03757188, -0.06533337, -0.03698472,  0.06051698, -0.03029809,\n",
       "         0.0885445 ,  0.01633714, -0.04648203, -0.00442379, -0.04743735,\n",
       "        -0.05824102, -0.01571286,  0.00104176,  0.08043814,  0.009137  ,\n",
       "         0.05208281, -0.01729153, -0.02269967,  0.01739281, -0.07773247,\n",
       "        -0.09463692, -0.00976295, -0.0434976 , -0.00954829,  0.0002124 ,\n",
       "         0.06618541,  0.05771648,  0.01892778,  0.0192249 ,  0.01761094,\n",
       "        -0.02053336,  0.02733704, -0.0652618 ,  0.00977596,  0.0184041 ,\n",
       "        -0.05372442,  0.05733688,  0.02478351, -0.04324061,  0.03130506,\n",
       "        -0.02435355,  0.01432731,  0.00075861,  0.03092475, -0.01801764,\n",
       "         0.02697023,  0.0400332 ,  0.06813575, -0.01820336, -0.05381026,\n",
       "         0.02869741,  0.07686509,  0.04766869, -0.02354925], dtype=float32)>,\n",
       " <tf.Variable 'dense_49/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.14842072, -0.14841785, -0.08558908, ..., -0.17940158,\n",
       "          0.14146301, -0.08850405],\n",
       "        [ 0.16668148, -0.14354724, -0.04517128, ...,  0.08479635,\n",
       "         -0.15682115,  0.0345397 ],\n",
       "        [-0.02803455,  0.13913345, -0.11594185, ...,  0.06276284,\n",
       "         -0.14456986,  0.16065562],\n",
       "        ...,\n",
       "        [ 0.18682897,  0.12891598, -0.26126772, ..., -0.17531541,\n",
       "          0.13393125, -0.16807799],\n",
       "        [-0.0869476 , -0.11922239,  0.06177285, ..., -0.15473405,\n",
       "          0.09449899, -0.12262658],\n",
       "        [-0.05935301, -0.00593273,  0.11325775, ...,  0.00620808,\n",
       "         -0.1015666 ,  0.02734643]], dtype=float32)>,\n",
       " <tf.Variable 'dense_49/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([-0.0238473 , -0.07199923, -0.00211789, -0.01138015, -0.00185321,\n",
       "        -0.02147076,  0.01312282,  0.05450632,  0.01458121,  0.08273447,\n",
       "        -0.08882371, -0.00502067, -0.04591109,  0.05900142,  0.02823539,\n",
       "         0.01209249, -0.01244395,  0.04418903,  0.03085488,  0.02624189,\n",
       "         0.00331342,  0.0134234 ,  0.00867402, -0.01742133,  0.        ,\n",
       "        -0.02219016, -0.0612294 , -0.03280583, -0.02894875, -0.05863471,\n",
       "         0.09046686,  0.04038045,  0.04865606,  0.11427468, -0.01461246,\n",
       "         0.06173771,  0.03097924,  0.00868528,  0.00646785, -0.02466561,\n",
       "        -0.00296162,  0.01841428,  0.00152502, -0.00537715,  0.01677805,\n",
       "        -0.04546919,  0.01790801, -0.01665894, -0.01509697, -0.02134386,\n",
       "        -0.01200761,  0.03026729,  0.02390174, -0.01295031,  0.00372669,\n",
       "         0.02888595, -0.00832452,  0.04384567,  0.01222725, -0.00606523,\n",
       "         0.02300359, -0.02493972, -0.00428025,  0.00287588], dtype=float32)>,\n",
       " <tf.Variable 'dense_50/kernel:0' shape=(64, 3) dtype=float32, numpy=\n",
       " array([[ 3.91538553e-02,  6.38222322e-02, -1.79838970e-01],\n",
       "        [-1.59814358e-01, -6.23731017e-02,  1.01004615e-01],\n",
       "        [-3.23075116e-01, -1.52369663e-01,  2.43967213e-02],\n",
       "        [-9.12740454e-02,  1.98980764e-01,  8.19643959e-02],\n",
       "        [-2.46340856e-01, -1.08572445e-03,  2.28192732e-01],\n",
       "        [ 2.31456429e-01, -9.76850241e-02,  1.76415756e-01],\n",
       "        [ 2.50703305e-01,  1.10569876e-02,  2.87520215e-02],\n",
       "        [ 2.33735591e-01, -9.46741179e-02, -4.66199219e-01],\n",
       "        [-2.92304635e-01, -2.39247888e-01,  1.11752383e-01],\n",
       "        [ 1.34203434e-01,  1.94705188e-01, -7.99020827e-02],\n",
       "        [-1.87014133e-01, -9.17200297e-02, -3.46938637e-03],\n",
       "        [-2.61450827e-01,  2.10105002e-01,  3.18430603e-01],\n",
       "        [-1.39023557e-01,  7.55173638e-02,  1.41461462e-01],\n",
       "        [ 4.31079883e-03,  3.08512539e-01, -3.51637721e-01],\n",
       "        [-2.83281535e-01,  4.42634672e-02,  1.25487208e-01],\n",
       "        [-2.61725694e-01,  9.08675045e-02,  2.42420897e-01],\n",
       "        [ 2.78953403e-01, -9.99224856e-02,  2.12311551e-01],\n",
       "        [ 2.37459883e-01,  5.27990311e-02, -2.73992330e-01],\n",
       "        [ 2.08988160e-01,  2.49621764e-01,  7.61661306e-02],\n",
       "        [-3.47991109e-01,  2.33288914e-01,  2.29236379e-01],\n",
       "        [ 1.21428452e-01, -1.50401801e-01,  7.25022703e-02],\n",
       "        [ 2.85104632e-01, -6.00119382e-02,  5.04839839e-03],\n",
       "        [ 2.04065427e-01, -2.72816867e-01,  1.85697913e-01],\n",
       "        [-2.07640082e-01, -8.96910056e-02,  1.01504117e-01],\n",
       "        [-2.77301073e-02, -1.49185404e-01, -1.45756304e-01],\n",
       "        [ 1.53468167e-02, -1.08976990e-01,  1.03040390e-01],\n",
       "        [ 2.55494177e-01,  9.63903144e-02, -9.91817191e-02],\n",
       "        [-4.90602642e-01, -1.63627043e-01,  2.89423257e-01],\n",
       "        [ 1.23188064e-01,  1.29772365e-01,  5.33906147e-02],\n",
       "        [-1.80525869e-01, -1.32735431e-01, -1.24277264e-01],\n",
       "        [ 2.51316279e-01,  2.08711445e-01, -3.61718982e-01],\n",
       "        [-8.50687549e-02,  2.40217671e-01, -2.70205706e-01],\n",
       "        [-5.59794419e-02,  3.56071852e-02, -2.19969615e-01],\n",
       "        [ 2.46492237e-01,  2.05625132e-01, -1.80353686e-01],\n",
       "        [-7.29592294e-02, -1.39711782e-01,  3.46677035e-01],\n",
       "        [ 9.44084749e-02,  2.09688079e-02, -2.95915365e-01],\n",
       "        [ 2.08460808e-01, -1.98906466e-01, -2.12682754e-01],\n",
       "        [ 1.23815440e-01, -2.55534768e-01, -2.82771856e-01],\n",
       "        [ 2.72616416e-01, -1.14688039e-01,  1.29080221e-01],\n",
       "        [ 4.59333062e-02, -1.09461613e-01,  1.73333809e-01],\n",
       "        [-2.39455834e-01, -2.80525774e-01, -1.06416732e-01],\n",
       "        [-6.48436099e-02,  4.10759188e-02,  2.22016379e-01],\n",
       "        [-4.82457504e-02, -1.27655193e-02,  1.97433650e-01],\n",
       "        [-1.40125111e-01,  2.35621072e-03,  7.56367818e-02],\n",
       "        [ 1.60069287e-01, -2.92903781e-01,  3.55401397e-01],\n",
       "        [ 3.99133302e-02, -4.24051397e-02,  3.33474785e-01],\n",
       "        [-1.66394576e-01,  1.25808030e-01, -2.08886564e-01],\n",
       "        [-1.36702314e-01, -9.66302752e-02,  2.40633413e-01],\n",
       "        [-4.21003290e-02, -1.61380440e-01,  7.47109130e-02],\n",
       "        [-6.29152358e-02,  1.14731498e-01,  2.72731662e-01],\n",
       "        [-1.39158934e-01, -2.43056059e-01,  2.90034562e-01],\n",
       "        [-3.76233518e-01, -2.04149440e-01,  4.05037217e-02],\n",
       "        [ 2.82858849e-01, -2.23131418e-01, -3.00085515e-01],\n",
       "        [ 1.60842910e-01,  1.67880446e-01,  1.81960061e-01],\n",
       "        [-1.53327301e-01, -1.79448977e-01,  7.73472637e-02],\n",
       "        [-2.68624246e-01,  1.45058170e-01, -4.12724316e-01],\n",
       "        [-1.25364736e-01, -2.08532974e-01,  3.15520585e-01],\n",
       "        [ 2.66081154e-01,  3.31668794e-01,  1.02302134e-01],\n",
       "        [ 4.36431146e-04, -2.94650644e-01,  3.52141424e-03],\n",
       "        [-2.46001661e-01,  2.71425068e-01,  4.30716537e-02],\n",
       "        [ 1.28410598e-02, -2.47139230e-01, -3.40142965e-01],\n",
       "        [-2.25420743e-01,  7.25009739e-02, -1.06492296e-01],\n",
       "        [ 3.94043475e-02, -2.80781031e-01, -8.02749768e-02],\n",
       "        [ 5.02312034e-02,  2.90105283e-01,  1.67492285e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense_50/bias:0' shape=(3,) dtype=float32, numpy=array([ 0.00909398,  0.03061567, -0.07822796], dtype=float32)>]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================].2973 - accuracy: 0.89 - 2s 1s/step - loss: 0.4491 - accuracy: 0.85 - 2s 738ms/step - loss: 0.4331 - accuracy: 0.854 - 2s 562ms/step - loss: 0.3985 - accuracy: 0.867 - 2s 454ms/step - loss: 0.4238 - accuracy: 0.862 - 2s 383ms/step - loss: 0.4013 - accuracy: 0.864 - 2s 333ms/step - loss: 0.4058 - accuracy: 0.863 - 2s 295ms/step - loss: 0.4235 - accuracy: 0.861 - 2s 266ms/step - loss: 0.4298 - accuracy: 0.862 - 2s 242ms/step - loss: 0.4815 - accuracy: 0.850 - 2s 223ms/step - loss: 0.4988 - accuracy: 0.845 - 2s 206ms/step - loss: 0.4833 - accuracy: 0.852 - 3s 193ms/step - loss: 0.5101 - accuracy: 0.851 - 3s 181ms/step - loss: 0.4947 - accuracy: 0.853 - 3s 171ms/step - loss: 0.5032 - accuracy: 0.852 - 3s 162ms/step - loss: 0.5162 - accuracy: 0.848 - 3s 154ms/step - loss: 0.5080 - accuracy: 0.851 - 3s 147ms/step - loss: 0.5044 - accuracy: 0.848 - 3s 141ms/step - loss: 0.4951 - accuracy: 0.848 - 3s 136ms/step - loss: 0.5028 - accuracy: 0.845 - 3s 130ms/step - loss: 0.5027 - accuracy: 0.844 - 3s 126ms/step - loss: 0.5075 - accuracy: 0.845 - 3s 122ms/step - loss: 0.5173 - accuracy: 0.845 - 3s 118ms/step - loss: 0.5167 - accuracy: 0.843 - 3s 114ms/step - loss: 0.5119 - accuracy: 0.841 - 3s 111ms/step - loss: 0.5130 - accuracy: 0.841 - 3s 108ms/step - loss: 0.5137 - accuracy: 0.842 - 3s 105ms/step - loss: 0.5131 - accuracy: 0.841 - 3s 103ms/step - loss: 0.5149 - accuracy: 0.840 - 3s 100ms/step - loss: 0.5146 - accuracy: 0.841 - 3s 98ms/step - loss: 0.5147 - accuracy: 0.840 - 3s 96ms/step - loss: 0.5086 - accuracy: 0.84 - 3s 94ms/step - loss: 0.5025 - accuracy: 0.84 - 3s 92ms/step - loss: 0.5087 - accuracy: 0.84 - 3s 90ms/step - loss: 0.5197 - accuracy: 0.84 - 3s 88ms/step - loss: 0.5251 - accuracy: 0.83 - 3s 87ms/step - loss: 0.5214 - accuracy: 0.83 - 3s 85ms/step - loss: 0.5233 - accuracy: 0.83 - 3s 84ms/step - loss: 0.5260 - accuracy: 0.83 - 3s 82ms/step - loss: 0.5216 - accuracy: 0.83 - 3s 81ms/step - loss: 0.5185 - accuracy: 0.83 - 3s 80ms/step - loss: 0.5195 - accuracy: 0.83 - 3s 79ms/step - loss: 0.5252 - accuracy: 0.83 - 3s 78ms/step - loss: 0.5239 - accuracy: 0.83 - 3s 77ms/step - loss: 0.5192 - accuracy: 0.83 - 3s 76ms/step - loss: 0.5130 - accuracy: 0.83 - 4s 75ms/step - loss: 0.5109 - accuracy: 0.83 - 4s 74ms/step - loss: 0.5063 - accuracy: 0.83 - 4s 73ms/step - loss: 0.5049 - accuracy: 0.83 - 4s 72ms/step - loss: 0.5018 - accuracy: 0.83 - 4s 71ms/step - loss: 0.5073 - accuracy: 0.83 - 4s 70ms/step - loss: 0.5030 - accuracy: 0.83 - 4s 69ms/step - loss: 0.5046 - accuracy: 0.83 - 4s 69ms/step - loss: 0.5002 - accuracy: 0.83 - 4s 68ms/step - loss: 0.4982 - accuracy: 0.83 - 4s 67ms/step - loss: 0.4944 - accuracy: 0.83 - 4s 66ms/step - loss: 0.4989 - accuracy: 0.83 - 4s 66ms/step - loss: 0.5026 - accuracy: 0.83 - 4s 65ms/step - loss: 0.5060 - accuracy: 0.83 - 4s 65ms/step - loss: 0.5065 - accuracy: 0.83 - 4s 64ms/step - loss: 0.5029 - accuracy: 0.83 - 4s 63ms/step - loss: 0.5022 - accuracy: 0.83 - 4s 63ms/step - loss: 0.4999 - accuracy: 0.83 - 4s 62ms/step - loss: 0.4998 - accuracy: 0.83 - 4s 62ms/step - loss: 0.4983 - accuracy: 0.83 - 4s 61ms/step - loss: 0.4965 - accuracy: 0.83 - 4s 61ms/step - loss: 0.4967 - accuracy: 0.83 - 4s 60ms/step - loss: 0.4980 - accuracy: 0.83 - 4s 60ms/step - loss: 0.4967 - accuracy: 0.83 - 4s 59ms/step - loss: 0.4941 - accuracy: 0.83 - 4s 59ms/step - loss: 0.4960 - accuracy: 0.83 - 4s 59ms/step - loss: 0.4982 - accuracy: 0.83 - 4s 58ms/step - loss: 0.4983 - accuracy: 0.83 - 4s 58ms/step - loss: 0.4979 - accuracy: 0.83 - 4s 57ms/step - loss: 0.4990 - accuracy: 0.83 - 4s 57ms/step - loss: 0.5016 - accuracy: 0.83 - 4s 57ms/step - loss: 0.5007 - accuracy: 0.83 - 4s 56ms/step - loss: 0.5009 - accuracy: 0.83 - 4s 56ms/step - loss: 0.4950 - accuracy: 0.83 - 5s 57ms/step - loss: 0.4950 - accuracy: 0.8326\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval loss: 0.495, Eval accuracy: 0.833\n"
     ]
    }
   ],
   "source": [
    "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([np.array([1,2,3])]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1113018, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.09003057, 0.24472848, 0.66524094],\n",
       "       [0.09003057, 0.24472848, 0.66524094]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(tf.constant([[1.,2.,3.],[4.,5.,6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [3,10,64] vs. [3,10,3] [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e68e9fd87d3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6695\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6696\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6697\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6698\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6699\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\django\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [3,10,64] vs. [3,10,3] [Op:Mul]"
     ]
    }
   ],
   "source": [
    "tf.multiply(tf.ones([3,10,64]),tf.zeros([3,10,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 10)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([tf.random.normal([3,10])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  채널 3개로 각 tiem stamp attention weight append하기\n",
    "#  [batch,attention_r,timestamp]\n",
    "#  [timestamp 차원 기준으로 sottmax 적용하기]\n",
    "#  tranpose 사용하여 [batch,timestamp,attion_r]\n",
    "#  for 문사용 multiply\n",
    "#  bi LSTM 결과와 multiply 함 attention_r과\n",
    "# tf.reduce_sum timestamp 기준으로 결과는 \n",
    "#  [batch,LSTM_hidden_unitsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1113371, shape=(10, 3, 1), dtype=float32, numpy=\n",
       "array([[[-0.17747594],\n",
       "        [-0.11517678],\n",
       "        [ 0.13129044]],\n",
       "\n",
       "       [[ 1.5023729 ],\n",
       "        [-1.1652751 ],\n",
       "        [-1.3593    ]],\n",
       "\n",
       "       [[ 0.18969494],\n",
       "        [-1.2070906 ],\n",
       "        [-0.6470972 ]],\n",
       "\n",
       "       [[-0.45896432],\n",
       "        [-2.1450145 ],\n",
       "        [-0.3252337 ]],\n",
       "\n",
       "       [[-0.11928146],\n",
       "        [ 0.3295987 ],\n",
       "        [-1.2945286 ]],\n",
       "\n",
       "       [[-2.1132977 ],\n",
       "        [ 0.78426933],\n",
       "        [ 0.28413895]],\n",
       "\n",
       "       [[-1.4640176 ],\n",
       "        [-0.77243257],\n",
       "        [-0.3816451 ]],\n",
       "\n",
       "       [[-1.2258996 ],\n",
       "        [ 1.6782123 ],\n",
       "        [ 0.22770302]],\n",
       "\n",
       "       [[ 0.5065589 ],\n",
       "        [ 0.45654002],\n",
       "        [-0.86232543]],\n",
       "\n",
       "       [[ 0.8482531 ],\n",
       "        [-0.4906019 ],\n",
       "        [-1.3737457 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(np.array([tf.random.normal([3,10])]),(2,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.keras.initializers.glorot_normal()\n",
    "a=tf.constant(init([10,10,128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "b =  tf.Variable(init([128,50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b_=tf.tile(tf.expand_dims(b,0),[10,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "In[0] mismatch In[1] shape: 128 vs. 256: [10,10,128] [10,256,100] 0 0 [Op:BatchMatMulV2] name: MatMul/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-7523bfb2dd59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m         \u001b[0madjoint_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2727\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mbatch_mat_mul_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madjoint_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madjoint_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m     \u001b[1;31m# Neither matmul nor sparse_matmul support adjoint, so we conjugate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mbatch_mat_mul_v2\u001b[1;34m(x, y, adj_x, adj_y, name)\u001b[0m\n\u001b[0;32m   1700\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1702\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1703\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madj_x\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\django\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: In[0] mismatch In[1] shape: 128 vs. 256: [10,10,128] [10,256,100] 0 0 [Op:BatchMatMulV2] name: MatMul/"
     ]
    }
   ],
   "source": [
    "tf.matmul(a,b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = tf.keras.Input(shape=(None,784))\n",
    "#shape = [ tf.shape(X)[k] for k in range(4)]\n",
    "#Y = tf.reshape(X , [shape[0], shape[1]*shape[2], shape[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [ tf.shape(X)[k] for k in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.reshape(X , [shape[0], shape[1]*shape[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=tf.constant([[[1.,5.,7.],[1.,2.,3.]],[[2.,5.,7.],[1.,2.,3.]]])\n",
    "b=tf.nn.softmax(a,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8475491, shape=(1, 3), dtype=float32, numpy=array([[-0.1343434 ,  0.0753022 ,  0.36052102]], dtype=float32)>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=init([1,3])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8475492, shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[0.3656566 , 1.0278764 , 1.3425348 ],\n",
       "        [0.3656566 , 0.12272807, 0.37850723]],\n",
       "\n",
       "       [[0.5967152 , 1.0278764 , 1.3425348 ],\n",
       "        [0.13459803, 0.12272807, 0.37850723]]], dtype=float32)>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5716, shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[0.5       , 0.95257413, 0.98201376],\n",
       "        [0.5       , 0.04742587, 0.01798621]],\n",
       "\n",
       "       [[0.7310586 , 0.95257413, 0.98201376],\n",
       "        [0.26894143, 0.04742587, 0.01798621]]], dtype=float32)>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.98201376 + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    # 첨 class 선언신 변수생성\n",
    "    def __init__(self, num_unit,input_units):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.num_unit = num_unit\n",
    "        self.input_units = input_units\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.kernel1 = self.add_weight(\"kernel1\",\n",
    "                    initializer=tf.keras.initializers.he_normal(seed=1337),                               \n",
    "                                  shape=[self.input_units,\n",
    "                                         self.num_unit])\n",
    "        self.kernel2 = self.add_weight(\"kernel2\",\n",
    "                                       shape =[self.num_unit])\n",
    "                    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        atten_total = tf.matmul(inputs[:,1,:],self.kernel1)+self.kernel2\n",
    "        lens=inputs.shape[1]\n",
    "        for i in range(1,lens):        \n",
    "            try:\n",
    "                atten = tf.matmul(inputs[:,i,:],self.kernel1)+self.kernel2\n",
    "                atten_total = tf.concat([atten,atten_total],axis=1)\n",
    "            except:\n",
    "                break\n",
    "        atten_weights_result = tf.nn.softmax(atten_total)\n",
    "        #return atten_weights_result\n",
    "        atten_weights_result = tf.expand_dims(atten_weights_result,2)\n",
    "        #atten_weights_result = tf.transpose(np.array([atten_weights_result.numpy()]),(1,2,0))\n",
    "        outputs = tf.multiply(inputs,atten_weights_result)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    # 첨 class 선언신 변수생성\n",
    "    def __init__(self,input_units, num_unit,output_unit,lams):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.num_unit = num_unit\n",
    "        self.output_unit = output_unit\n",
    "        self.lambdas = lams\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.kernel1 = self.add_weight(\"kernel1\",\n",
    "                    initializer = tf.keras.initializers.glorot_normal(),\n",
    "                    regularizer = tf.keras.regularizers.l2(self.lambdas),\n",
    "                    shape =[self.input_units,self.num_unit])\n",
    "        self.bias1 = self.add_weight(\"bias1\",initializer = tf.zeros_initializer(),shape=[1,self.num_unit])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.kernel2 = self.add_weight(\"kernel2\",\n",
    "                    initializer = tf.keras.initializers.glorot_normal(),\n",
    "                    regularizer = tf.keras.regularizers.l2(self.lambdas),\n",
    "                    shape = [self.num_unit,self.output_unit])\n",
    "        self.bias2 = self.add_weight(\"bias2\",initializer = tf.zeros_initializer(),shape=[1,self.output_unit])\n",
    "        \n",
    "                    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        shape = [ tf.shape(inputs)[k] for k in range(len(inputs.shape))]\n",
    "        \n",
    "        shape_list = [shape[0]] + [1 for i in range(len(inputs.shape)-1)]\n",
    "        kernel1_ = tf.tile(tf.expand_dims(self.kernel1,0),shape_list)\n",
    "        kernel2_ = tf.tile(tf.expand_dims(self.kernel2,0),shape_list)\n",
    "        \n",
    "        h1 = tf.matmul(inputs,kernel1_) + self.bias1\n",
    "        h1 = tf.nn.tanh(h1)\n",
    "        out = tf.matmul(h1,kernel2_) + self.bias2\n",
    "        out = tf.nn.softmax(out,axis=1)\n",
    "        flat_output_total = tf.multiply(inputs,tf.expand_dims(out[:,:,0],axis=2))\n",
    "        flat_output_total = tf.reduce_sum(flat_output_total,axis=1)\n",
    "        for i in range(1,self.output_unit):\n",
    "            flat_output= tf.multiply(inputs,tf.expand_dims(out[:,:,i],axis=2))\n",
    "            flat_output = tf.reduce_sum(flat_output,axis=1)\n",
    "            flat_output_total = tf.concat([flat_output_total,flat_output],axis=1)\n",
    "        \n",
    "        return flat_output_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = AttentionLayer(128,50,5,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dd=cc(init([50,10,128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "\n",
    "        self.init = tf.keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = tf.keras.regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = tf.keras.regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = tf.keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = tf.keras.constraints.get(W_constraint)\n",
    "        self.u_constraint = tf.keras.constraints.get(u_constraint)\n",
    "        self.b_constraint = tf.keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(shape = (input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape = (input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        #self.u에 대한 input shape 조정\n",
    "        self.u = self.add_weight(shape = (input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        uit = tf.tensordot(x, self.W,1)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = tf.keras.activations.tanh(uit)\n",
    "        ait = tf.tensordot(uit, self.u,1)\n",
    "        a = tf.keras.activations.softmax(ait)\n",
    "        a = tf.expand_dims(a,-1)\n",
    "        weighted_input = x * a\n",
    "\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
